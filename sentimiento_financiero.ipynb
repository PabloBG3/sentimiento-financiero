{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Trabajo Fin de Máster  \n",
        "## Análisis de sentimientos en noticias financieras mediante Aprendizaje Automático y Procesamiento del Lenguaje Natural  \n",
        "\n",
        "\n",
        "*   ### Autor: Pablo Bayón Gala\n",
        "*   ### Universidad: Universidad Internacional de La Rioja\n",
        "*   ### Máster: Máster Universitario en Inteligencia Artificial  \n",
        "*   ### Fecha: Septiembre 2025\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "© 2025 Pablo Bayón Gala.\n",
        "\n",
        "Este cuaderno contiene el código y los experimentos realizados para el desarrollo del Trabajo Fin de Máster, en el que se implementan y comparan distintos modelos de Procesamiento del Lenguaje Natural aplicados al análisis de sentimiento en textos financieros.\n",
        "\n",
        "Se distribuye bajo la **GNU General Public License v3.0 (GPL v3)**.\n",
        "\n",
        "Para más detalles sobre la licencia: [https://www.gnu.org/licenses/gpl-3.0.html](https://www.gnu.org/licenses/gpl-3.0.html)\n"
      ],
      "metadata": {
        "id": "5UHJHXR-pyFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento de los datos"
      ],
      "metadata": {
        "id": "KsVOAzNd1-37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El preprocesamiento de datos siguiente lo voy a estructurar en forma de pipeline diferenciando dos tipos:\n",
        "\n",
        "1. Por un lado, los tres modelos basados en arquitectura Transformers (FinBERT, DistilRoBERTa con fine-tuning y BERTweet) ya incluyen un tokenizador preentrenado junto con el modelo. Dicho tokenizador espera el texto en bruto (con mayúsculas, tildes, hashtags, etc.) porque su preentrenamiento se realizó con texto real provenientes de noticias / tweets. Para estos modelos, se implementa una limpieza ligera para no alterar el lenguaje natural. Su flujo de limpieza consiste en los siguientes pasos:\n",
        "  * Eliminación de duplicados\n",
        "  *\tEliminación de tweets vacíos\n",
        "  *\tEliminación de URLs\n",
        "\n",
        "2. Por otro lado, el modelo basado en diccionarios y reglas (VADER) requiere un preprocesamiento más profundo. Su flujo de limpieza se basa en los siguientes pasos:\n",
        "  *\tEliminación de duplicados\n",
        "  *\tEliminación de tweets vacíos\n",
        "  *\tEliminación de URLs\n",
        "  *\tEliminación de menciones (@usuario)\n",
        "  *\tEliminación de hashtags\n",
        "  *\tEliminación de espacios adicionales"
      ],
      "metadata": {
        "id": "uC3ephrypzvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Pipeline de preprocesamiento para modelos basados en Transformers \"\"\"\n",
        "def limpieza_transformers(texto):\n",
        "\n",
        "  # Eliminamos URLs\n",
        "  texto = re.sub(r\"(http[s]?://\\S+)|(www\\.\\S+)\", \"\", texto)\n",
        "\n",
        "  return texto\n",
        "\n",
        "\"\"\" Pipeline de preprocesamiento para VADER \"\"\"\n",
        "def limpieza_vader(texto):\n",
        "\n",
        "  # Eliminamos menciones\n",
        "  texto = re.sub('@[^\\s]+','',texto)\n",
        "  # Eliminamos hashtags (manteniendo la palabra del hashtag)\n",
        "  texto = re.sub(r'#','',texto)\n",
        "  # Eliminamos URLs\n",
        "  texto = re.sub(r\"(http[s]?://\\S+)|(www\\.\\S+)\", \"\", texto)\n",
        "  # Reemplazamos múltiples espacios seguidos por un solo espacio\n",
        "  texto = re.sub(r'\\s+', ' ', texto, flags=re.I)\n",
        "\n",
        "  return texto"
      ],
      "metadata": {
        "id": "RedrWaWO2Viv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluación de mis modelos PLN a partir de los conjuntos de datos etiquetados"
      ],
      "metadata": {
        "id": "DD7JpXgdwnzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset sobre titulares de noticias financieras\n",
        "**Labeled Stock News Headlines**\n",
        "\n",
        "URL: https://www.kaggle.com/datasets/johoetter/labeled-stock-news-headlines"
      ],
      "metadata": {
        "id": "lZ4aNiWExW-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Cargo el dataset\n",
        "df_Stock_News_Headlines = pd.read_csv('stock_news.csv')\n",
        "\n",
        "df_Stock_News_Headlines.head()"
      ],
      "metadata": {
        "id": "AbRaDooXx9WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Stock_News_Headlines.shape"
      ],
      "metadata": {
        "id": "czW7mFrGyCei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Stock_News_Headlines.info()"
      ],
      "metadata": {
        "id": "RAr3V59dyF_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Datos\n",
        "counts = df_Stock_News_Headlines['label'].value_counts()\n",
        "labels = counts.index\n",
        "\n",
        "# Paleta de colores agradable (pastel o profesional)\n",
        "colors = plt.cm.Pastel1(range(len(labels)))\n",
        "\n",
        "# Gráfico\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "wedges, texts, autotexts = ax.pie(\n",
        "    counts,\n",
        "    labels=labels,\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    colors=colors,\n",
        "    textprops={'fontsize': 12, 'color': 'black'}\n",
        ")\n",
        "\n",
        "# Mejorar visibilidad de porcentajes\n",
        "for autotext in autotexts:\n",
        "    autotext.set_color('black')\n",
        "    autotext.set_fontweight('bold')\n",
        "\n",
        "# Título con formato profesional\n",
        "ax.set_title(\n",
        "    \"Distribución de sentimiento en dataset original\",\n",
        "    fontsize=12,\n",
        "    fontweight='bold',\n",
        "    pad=20\n",
        ")\n",
        "\n",
        "# Mantener el pastel como círculo perfecto\n",
        "ax.axis('equal')\n",
        "\n",
        "# Quitar el borde de las etiquetas para un look limpio\n",
        "plt.setp(texts, fontweight='medium')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7X1k9Vy2yIM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BALANCEAMOS EL DATASET\n",
        "# Comprobamos distribución inicial\n",
        "print(\"Distribución original:\")\n",
        "print(df_Stock_News_Headlines['label'].value_counts(normalize=True) * 100)\n",
        "\n",
        "# Determinamos tamaño de la clase minoritaria (Negative)\n",
        "min_class = \"Negative\"\n",
        "min_count = df_Stock_News_Headlines[df_Stock_News_Headlines['label'] == min_class].shape[0]\n",
        "print(f\"\\nTamaño de la clase minoritaria ({min_class}): {min_count}\")\n",
        "\n",
        "# Tomamos todos los textos de la clase minoritaria\n",
        "df_min = df_Stock_News_Headlines[df_Stock_News_Headlines['label'] == min_class]\n",
        "\n",
        "# Submuestreamos aleatoriamente las otras clases para igualar tamaño\n",
        "balanced_parts = [df_min]  # empezamos con la clase minoritaria\n",
        "\n",
        "for label in [\"Positive\", \"Neutral\"]:\n",
        "    df_label = df_Stock_News_Headlines[df_Stock_News_Headlines['label'] == label].sample(\n",
        "        n=min_count,\n",
        "        random_state=42\n",
        "    )\n",
        "    balanced_parts.append(df_label)\n",
        "\n",
        "# Combinamos en un dataset balanceado\n",
        "df_Stock_News_Headlines_balanceado = pd.concat(balanced_parts).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Comprobamos distribución final\n",
        "print(\"\\nDistribución balanceada:\")\n",
        "print(df_Stock_News_Headlines_balanceado['label'].value_counts())\n",
        "\n",
        "# Guardamos el dataset balanceado\n",
        "df_Stock_News_Headlines_balanceado.to_excel(\"stock_news_balanced.xlsx\", index=False)\n",
        "\n",
        "print(\"\\nDataset balanceado guardado correctamente.\")"
      ],
      "metadata": {
        "id": "FqCfJsccyQP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Stock_News_Headlines_balanceado.shape"
      ],
      "metadata": {
        "id": "JL8DZiVHytzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Modelo FinBERT"
      ],
      "metadata": {
        "id": "VJ1eMiPry9Lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Edw3fm-go_XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
        "\n",
        "# Cargar FinBERT desde Hugging Face\n",
        "model_name = \"ProsusAI/finbert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Crear el pipeline de FinBERT\n",
        "finbert = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=False, truncation=True)\n",
        "\n",
        "# Aplicar FinBERT a cada titular\n",
        "df_Stock_News_Headlines_balanceado['FinBERT_sentiment'] = df_Stock_News_Headlines_balanceado['headline'].apply(lambda x: finbert(x)[0]['label'])\n",
        "df_Stock_News_Headlines_balanceado['confidence'] = df_Stock_News_Headlines_balanceado['headline'].apply(lambda x: finbert(x)[0]['score'])\n",
        "\n",
        "# Mostrar resultados\n",
        "print(df_Stock_News_Headlines_balanceado)\n",
        "df_Stock_News_Headlines_balanceado.to_excel(\"LSNH_balanceado_FinBERT.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "RzAg2-Vlyw6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Vemos la arquitectura de alto nivel del modelo\n",
        "print(model)"
      ],
      "metadata": {
        "id": "HopyQb-EpM9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Accedemos a la configuración del modelo\n",
        "print(model.config)"
      ],
      "metadata": {
        "id": "XaPbQwckpPbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Asegurarse de que las etiquetas estén en el mismo formato (ej. capitalización)\n",
        "df_Stock_News_Headlines[\"label\"] = df_Stock_News_Headlines[\"label\"].str.lower().str.strip()\n",
        "df_Stock_News_Headlines[\"sentiment\"] = df_Stock_News_Headlines[\"sentiment\"].str.lower().str.strip()\n",
        "\n",
        "# Calcular matriz de confusión\n",
        "cm = confusion_matrix(df_Stock_News_Headlines[\"label\"], df_Stock_News_Headlines[\"sentiment\"], labels=[\"positive\", \"negative\", \"neutral\"])\n",
        "\n",
        "# Mostrar como DataFrame\n",
        "cm_df_Stock_News_Headlines = pd.DataFrame(cm, index=[\"True_Positive\", \"True_Negative\", \"True_Neutral\"],\n",
        "                        columns=[\"Pred_Positive\", \"Pred_Negative\", \"Pred_Neutral\"])\n",
        "\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df_Stock_News_Headlines)\n",
        "\n",
        "# Reporte de métricas\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(df_Stock_News_Headlines[\"label\"], df_Stock_News_Headlines[\"sentiment\"], digits=3))\n",
        "\n",
        "# Visualización con heatmap\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Pred_Positive\", \"Pred_Negative\", \"Pred_Neutral\"],\n",
        "            yticklabels=[\"True_Positive\", \"True_Negative\", \"True_Neutral\"])\n",
        "plt.xlabel(\"PREDICCIONES\")\n",
        "plt.ylabel(\"VALORES REALES\")\n",
        "plt.title(\"Matriz de Confusión (Dataset Noticias - Balanceado)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z1EW_8L5zL7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Modelo DistilRoBERTa + Fine-tuning"
      ],
      "metadata": {
        "id": "qHN3Tyg2zihK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Cargar el modelo desde Hugging Face\n",
        "model_name = \"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Creamos el pipeline con el modelo\n",
        "sentiment_model = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
        ")\n",
        "\n",
        "# Función para analizar sentimiento\n",
        "def analyze_sentiment(text):\n",
        "    result = sentiment_model(text)[0]\n",
        "    return pd.Series([result['label'], result['score']])\n",
        "\n",
        "# Aplicar análisis a los titulares\n",
        "df_Stock_News_Headlines_balanceado[['DistilRoBERTa_sentiment', 'confidence']] = df_Stock_News_Headlines_balanceado['headline'].apply(analyze_sentiment)\n",
        "\n",
        "print(df_Stock_News_Headlines_balanceado)\n",
        "df_Stock_News_Headlines_balanceado.to_excel(\"LSNH_balanceado_DistilRoBERTa.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "MaZNJbNMzmHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Vemos la arquitectura de alto nivel del modelo\n",
        "print(model)"
      ],
      "metadata": {
        "id": "lG0tx56jpVn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Accedemos a la configuración del modelo\n",
        "print(model.config)"
      ],
      "metadata": {
        "id": "N47jptm0pWrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Asegurarse de que las etiquetas estén en el mismo formato (ej. capitalización)\n",
        "df_Stock_News_Headlines_balanceado[\"label\"] = df_Stock_News_Headlines_balanceado[\"label\"].str.lower().str.strip()\n",
        "df_Stock_News_Headlines_balanceado[\"DistilRoBERTa_sentiment\"] = df_Stock_News_Headlines_balanceado[\"DistilRoBERTa_sentiment\"].str.lower().str.strip()\n",
        "\n",
        "# Calcular matriz de confusión\n",
        "cm = confusion_matrix(df_Stock_News_Headlines_balanceado[\"label\"], df_Stock_News_Headlines_balanceado[\"DistilRoBERTa_sentiment\"], labels=[\"positive\", \"negative\", \"neutral\"])\n",
        "\n",
        "# Mostrar como DataFrame\n",
        "cm_df_Stock_News_Headlines_balanceado = pd.DataFrame(cm, index=[\"True_Positive\", \"True_Negative\", \"True_Neutral\"],\n",
        "                        columns=[\"Pred_Positive\", \"Pred_Negative\", \"Pred_Neutral\"])\n",
        "\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df_Stock_News_Headlines_balanceado)\n",
        "\n",
        "# Reporte de métricas\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(df_Stock_News_Headlines_balanceado[\"label\"], df_Stock_News_Headlines_balanceado[\"DistilRoBERTa_sentiment\"], digits=3))\n",
        "\n",
        "# Visualización con heatmap\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Pred_Positive\", \"Pred_Negative\", \"Pred_Neutral\"],\n",
        "            yticklabels=[\"True_Positive\", \"True_Negative\", \"True_Neutral\"])\n",
        "plt.xlabel(\"PREDICCIONES\")\n",
        "plt.ylabel(\"VALORES REALES\")\n",
        "plt.title(\"Matriz de Confusión (Dataset Noticias - Balanceado)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l3GY-E9bzrsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Modelo BERTweet"
      ],
      "metadata": {
        "id": "RCB80CAYz2CZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar el modelo y tokenizer de BERTweet\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Modelo fine-tuned para análisis de sentimiento\n",
        "MODEL_NAME = \"finiteautomata/bertweet-base-sentiment-analysis\" # Este modelo ya está afinado específicamente para clasificación de sentimiento en tweets\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Creamos una función para predecir sentimiento\n",
        "def predict_sentiment(tweet: str) -> str:\n",
        "    inputs = tokenizer(tweet, return_tensors=\"pt\", truncation=True)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        label_id = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "    labels = model.config.id2label\n",
        "    return labels[label_id]\n",
        "\n",
        "# Lo aplico a mis tweets en formato original (sin preprocesado) porque el modelo de Hugging Face está entrenado para interpretar todo (emojis, menciones, etc.) como señales de sentimiento\n",
        "tqdm.pandas() # Para mostrar una barra de progreso\n",
        "# Convierto los valores a cadenas y reemplazo los NaN por texto vacío\n",
        "df_Stock_News_Headlines_balanceado['headline'] = df_Stock_News_Headlines_balanceado['headline'].fillna('').astype(str)\n",
        "# Creamos una nueva columna llamada bertweet_sentiment con etiquetas como NEG, NEU, POS llamando a la función anterior\n",
        "df_Stock_News_Headlines_balanceado['BERTweet_sentiment'] = df_Stock_News_Headlines_balanceado['headline'].progress_apply(predict_sentiment)\n",
        "\n",
        "# Guardamos los resultados\n",
        "df_Stock_News_Headlines_balanceado.to_excel(\"LSNH_balanceado_BERTweet.xlsx\", index=False)\n",
        "df_Stock_News_Headlines_balanceado.head()"
      ],
      "metadata": {
        "id": "XDjHQ3emz4MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Vemos la arquitectura de alto nivel del modelo\n",
        "print(model)"
      ],
      "metadata": {
        "id": "q0TNWa45qMtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Accedemos a la configuración del modelo\n",
        "print(model.config)"
      ],
      "metadata": {
        "id": "iNsZ8_hZqQaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tengo que mapear la etiqueta de sentiminto generada por mi modelo\n",
        "def map_labels(label):\n",
        "    if label == \"POS\":\n",
        "        return \"POSITIVE\"\n",
        "    elif label == \"NEG\":\n",
        "        return \"NEGATIVE\"\n",
        "    else:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "df_Stock_News_Headlines_balanceado[\"BERTweet_sentiment_mapped\"] = df_Stock_News_Headlines_balanceado[\"BERTweet_sentiment\"].apply(map_labels)\n",
        "\n",
        "df_Stock_News_Headlines_balanceado.head()"
      ],
      "metadata": {
        "id": "rtd-hUCW0IOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Asegurarse de que las etiquetas estén en el mismo formato (ej. capitalización)\n",
        "df_Stock_News_Headlines_balanceado[\"label\"] = df_Stock_News_Headlines_balanceado[\"label\"].str.lower().str.strip()\n",
        "df_Stock_News_Headlines_balanceado[\"BERTweet_sentiment_mapped\"] = df_Stock_News_Headlines_balanceado[\"BERTweet_sentiment_mapped\"].str.lower().str.strip()\n",
        "\n",
        "# Calcular matriz de confusión\n",
        "cm = confusion_matrix(df_Stock_News_Headlines_balanceado[\"label\"], df_Stock_News_Headlines_balanceado[\"BERTweet_sentiment_mapped\"], labels=[\"positive\", \"negative\", \"neutral\"])\n",
        "\n",
        "# Mostrar como DataFrame\n",
        "cm_df_Stock_News_Headlines_balanceado = pd.DataFrame(cm, index=[\"True_Positive\", \"True_Negative\", \"True_Neutral\"],\n",
        "                        columns=[\"Pred_Positive\", \"Pred_Negative\", \"Pred_Neutral\"])\n",
        "\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df_Stock_News_Headlines_balanceado)\n",
        "\n",
        "# Reporte de métricas\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(df_Stock_News_Headlines_balanceado[\"label\"], df_Stock_News_Headlines_balanceado[\"BERTweet_sentiment_mapped\"], digits=3))\n",
        "\n",
        "# Visualización con heatmap\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Pred_Positive\", \"Pred_Negative\", \"Pred_Neutral\"],\n",
        "            yticklabels=[\"True_Positive\", \"True_Negative\", \"True_Neutral\"])\n",
        "plt.xlabel(\"PREDICCIONES\")\n",
        "plt.ylabel(\"VALORES REALES\")\n",
        "plt.title(\"Matriz de Confusión (Dataset Noticias - Balanceado)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqHBEHAQ0NLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Modelo VADER"
      ],
      "metadata": {
        "id": "G4j-MvdW0Q33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalo librerías necesarias\n",
        "!pip install pyspellchecker\n",
        "!pip install scattertext\n",
        "!pip install nltk\n",
        "!pip install -U kaleido"
      ],
      "metadata": {
        "id": "6igNpSJr0N2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Data Preprocessing and Wrangling libraries\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import dateutil.parser\n",
        "\n",
        "# Import NLP Libraries\n",
        "import nltk\n",
        "from spellchecker import SpellChecker\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
        "\n",
        "# Import Visualization Libraries\n",
        "import plotly.offline as pyo\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "from plotly.subplots import make_subplots\n",
        "import seaborn as sns\n",
        "import scattertext as st\n",
        "from IPython.display import IFrame\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "import random\n",
        "\n",
        "# Downloading periphrals\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "U7GxYyjT0XXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializo herramientas\n",
        "\n",
        "# Para visualizaciones con seaborn\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "# Análisis de sentimiento con VADER\n",
        "sia = SIA()\n",
        "\n",
        "# Corrector ortográfico (puede ayudar antes del análisis de sentimiento)\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Para mostrar gráficos Plotly en notebooks\n",
        "pyo.init_notebook_mode()"
      ],
      "metadata": {
        "id": "DGHr27Dt0g8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Aplicamos pipeline de preprocesamiento para VADER (limpieza más profunda)\"\"\"\n",
        "\n",
        "# Se crea una copia del DataFrame original df_tweets para no modificarlo directamente\n",
        "data = df_Stock_News_Headlines_balanceado.copy()\n",
        "# Se añade una columna original_tweet con el texto sin procesar a modo de backup para comparar\n",
        "data['original_headline'] = df_Stock_News_Headlines_balanceado['headline']\n",
        "# Reemplaza los valores nulos (NaN) en la columna \"text\" por cadenas vacías '' y convierte todo el contenido de la columna \"text\" a tipo string.\n",
        "# Esto sirve para evitar que el modelo falle al encontrarse con un NaN y para garantizar que todo se maneje como string.\n",
        "data['headline'] = data['headline'].fillna('').astype(str)\n",
        "\n",
        "# Aplica la función de limpieza a la columna \"text\"\n",
        "data['headline'] = data['headline'].apply(limpieza_vader)\n",
        "# Eliminamos duplicados\n",
        "data.drop_duplicates(subset=[\"headline\"], inplace=True)\n",
        "# Reseteamos el índice por si se ha eliminado alguna fila\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "BzktNWKz0lJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "# Ancho máximo de cada línea (ejemplo: 80 caracteres)\n",
        "width = 80\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"headline ANTES de preprocesamiento:\")\n",
        "print(\"=\"*80)\n",
        "print(textwrap.fill(data.original_headline[0], width=width))  # Aquí hace el salto de línea\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "#print(data.original_tweet[0])\n",
        "print(\"headline DESPUÉS de preprocesamiento:\")\n",
        "print(80*\"=\")\n",
        "print(textwrap.fill(data.headline[0], width=width))  # También aquí\n",
        "#print(data.text[0])"
      ],
      "metadata": {
        "id": "eyumX5v01YHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esta función convierte un valor de sentimiento (entre -1 y 1, generado por VADER) en una etiqueta categórica\n",
        "def label_sentiment(x:float):\n",
        "    if x < -0.05 : return 'negative'  # negative si es muy bajo\n",
        "    if x > 0.05 : return 'positive'   # positive si es alto\n",
        "    return 'neutral'                  # neutral si está entre ambos umbrales\n",
        "\n",
        "# EXTRACCIÓN DE CARACTERÍSTICAS del texto\n",
        "# Extrae todas las palabras de cada tweet usando regex, eliminando puntuación.\n",
        "data['words'] = data.headline.apply(lambda x:re.findall(r'\\w+', x ))\n",
        "# Usa SpellChecker para detectar palabras mal escritas\n",
        "data['errors'] = data.words.apply(spell.unknown)\n",
        "# Cuenta cuántos errores ortográficos y cuántas palabras hay por tweet\n",
        "data['errors_count'] = data.errors.apply(len)\n",
        "data['words_count'] = data.words.apply(len)\n",
        "# Longitud del tweet (en caracteres)\n",
        "data['sentence_length'] = data.headline.apply(len)\n",
        "\n",
        "# Análisis de sentimiento para cada tweet\n",
        "# Aplica VADER (SentimentIntensityAnalyzer = SIA) a cada tweet para obtener el sentimiento compuesto (compound score), que va de -1 (negativo) a 1 (positivo).\n",
        "data['compound'] = [sia.polarity_scores(x)['compound'] for x in tqdm(data['headline'])] # Se usa tqdm para mostrar una barra de progreso\n",
        "# Clasifica el sentimiento numérico (compound) en positive, neutral o negative con la función definida al inicio.\n",
        "data['VADER_sentiment'] = data['compound'].apply(label_sentiment);\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "zGyQ1LyM1bOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos los resultados\n",
        "data.to_excel(\"LSNH_balanceado_VADER.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "NhEnJDy11evB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Asegurarse de que las etiquetas estén en el mismo formato (ej. capitalización)\n",
        "data[\"label\"] = data[\"label\"].str.lower().str.strip()\n",
        "data[\"VADER_sentiment\"] = data[\"VADER_sentiment\"].str.lower().str.strip()\n",
        "\n",
        "# Calcular matriz de confusión\n",
        "cm = confusion_matrix(data[\"label\"], data[\"VADER_sentiment\"], labels=[\"positive\", \"negative\", \"neutral\"])\n",
        "\n",
        "# Mostrar como DataFrame\n",
        "cm_data = pd.DataFrame(cm, index=[\"True_Positive\", \"True_Negative\", \"True_Neutral\"],\n",
        "                        columns=[\"Pred_Positive\", \"Pred_Negative\", \"Pred_Neutral\"])\n",
        "\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_data)\n",
        "\n",
        "# Reporte de métricas\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(data[\"label\"], data[\"VADER_sentiment\"], digits=3))\n",
        "\n",
        "# Visualización con heatmap\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Pred_Positive\", \"Pred_Negative\", \"Pred_Neutral\"],\n",
        "            yticklabels=[\"True_Positive\", \"True_Negative\", \"True_Neutral\"])\n",
        "plt.xlabel(\"PREDICCIONES\")\n",
        "plt.ylabel(\"VALORES REALES\")\n",
        "plt.title(\"Matriz de Confusión (Dataset Noticias - Balanceado)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0OEkE6nX1faD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset sobre tweets de carácter financiero\n",
        "**Financial Tweets Sentiment**\n",
        "\n",
        "URL: https://huggingface.co/datasets/TimKoornstra/financial-tweets-sentiment"
      ],
      "metadata": {
        "id": "qaLaS6-Q3gHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Cargar el dataset\n",
        "df_financial_tweets_sentiment = pd.read_parquet(\"train-00000-of-00001.parquet\")\n",
        "\n",
        "df_financial_tweets_sentiment.head()"
      ],
      "metadata": {
        "id": "QYNsAuHW3_I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_financial_tweets_sentiment.shape"
      ],
      "metadata": {
        "id": "LiOoAiVY4H_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_financial_tweets_sentiment.info()"
      ],
      "metadata": {
        "id": "Gv_pfV994J_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizo un mapeo de etiquetas para visualizar mejor los datos\n",
        "def map_labels(label):\n",
        "    if label == 1:\n",
        "        return \"Bullish\"   # Bullish\n",
        "    elif label == 2:\n",
        "        return \"Bearish\"   # Bearish\n",
        "    else:\n",
        "        return \"Neutral\"    # Neutral\n",
        "\n",
        "df_financial_tweets_sentiment[\"sentiment_mapped\"] = df_financial_tweets_sentiment[\"sentiment\"].apply(map_labels)\n",
        "\n",
        "df_financial_tweets_sentiment.head()"
      ],
      "metadata": {
        "id": "qbjm7B444MLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Datos\n",
        "counts = df_financial_tweets_sentiment['sentiment_mapped'].value_counts()\n",
        "labels = counts.index\n",
        "\n",
        "# Paleta de colores agradable (pastel o profesional)\n",
        "colors = plt.cm.Pastel1(range(len(labels)))\n",
        "\n",
        "# Gráfico\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "wedges, texts, autotexts = ax.pie(\n",
        "    counts,\n",
        "    labels=labels,\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    colors=colors,\n",
        "    textprops={'fontsize': 12, 'color': 'black'}\n",
        ")\n",
        "\n",
        "# Mejorar visibilidad de porcentajes\n",
        "for autotext in autotexts:\n",
        "    autotext.set_color('black')\n",
        "    autotext.set_fontweight('bold')\n",
        "\n",
        "# Título con formato profesional\n",
        "ax.set_title(\n",
        "    \"Distribución de sentimiento en dataset original\",\n",
        "    fontsize=12,\n",
        "    fontweight='bold',\n",
        "    pad=20\n",
        ")\n",
        "\n",
        "# Mantener el pastel como círculo perfecto\n",
        "ax.axis('equal')\n",
        "\n",
        "# Quitar el borde de las etiquetas para un look limpio\n",
        "plt.setp(texts, fontweight='medium')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PKKzDr-u4PKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BALANCEO EL DATASET\n",
        "# Comprobamos distribución inicial\n",
        "print(\"Distribución original:\")\n",
        "print(df_financial_tweets_sentiment['sentiment_mapped'].value_counts(normalize=True) * 100)\n",
        "\n",
        "# Determinamos tamaño de la clase minoritaria (Bearish)\n",
        "min_class = \"Bearish\"\n",
        "min_count = df_financial_tweets_sentiment[df_financial_tweets_sentiment['sentiment_mapped'] == min_class].shape[0]\n",
        "print(f\"\\nTamaño de la clase minoritaria ({min_class}): {min_count}\")\n",
        "\n",
        "# Tomamos todos los textos de la clase minoritaria\n",
        "df_min = df_financial_tweets_sentiment[df_financial_tweets_sentiment['sentiment_mapped'] == min_class]\n",
        "\n",
        "# Submuestreamos aleatoriamente las otras clases para igualar tamaño\n",
        "balanced_parts = [df_min]  # empezamos con la clase minoritaria\n",
        "\n",
        "for label in [\"Bullish\", \"Neutral\"]:\n",
        "    df_label = df_financial_tweets_sentiment[df_financial_tweets_sentiment['sentiment_mapped'] == label].sample(\n",
        "        n=min_count,\n",
        "        random_state=42\n",
        "    )\n",
        "    balanced_parts.append(df_label)\n",
        "\n",
        "# Combinamos en un dataset balanceado\n",
        "df_financial_tweets_sentiment_balanceado = pd.concat(balanced_parts).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Comprobamos distribución final\n",
        "print(\"\\nDistribución balanceada:\")\n",
        "print(df_financial_tweets_sentiment_balanceado['sentiment_mapped'].value_counts())\n",
        "\n",
        "# Guardamos el dataset balanceado\n",
        "df_financial_tweets_sentiment_balanceado.to_excel(\"Dataset_financial_tweets_sentiment_balanced.xlsx\", index=False)\n",
        "\n",
        "print(\"\\nDataset balanceado guardado correctamente.\")"
      ],
      "metadata": {
        "id": "oy7vrNdL4TTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizo un mapeo de etiquetas antes de comparar, para así usar exactamente el mismo formato que generan mis modelos\n",
        "def map_labels(label):\n",
        "    if label == 1:\n",
        "        return \"POSITIVE\"   # Bullish\n",
        "    elif label == 2:\n",
        "        return \"NEGATIVE\"   # Bearish\n",
        "    else:\n",
        "        return \"NEUTRAL\"    # Neutral\n",
        "\n",
        "df_financial_tweets_sentiment_balanceado[\"sentiment_mapped\"] = df_financial_tweets_sentiment_balanceado[\"sentiment\"].apply(map_labels)\n",
        "\n",
        "df_financial_tweets_sentiment_balanceado.head()"
      ],
      "metadata": {
        "id": "nbsh-Bb94iGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Modelo FinBERT"
      ],
      "metadata": {
        "id": "vkWf_f4C4qfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
        "\n",
        "# Cargar FinBERT desde Hugging Face\n",
        "model_name = \"ProsusAI/finbert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Crear el pipeline de FinBERT\n",
        "finbert = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=False, truncation=True)\n",
        "\n",
        "# Aplicar FinBERT a cada titular\n",
        "df_financial_tweets_sentiment_balanceado['FinBERT_sentiment'] = df_financial_tweets_sentiment_balanceado['tweet'].apply(lambda x: finbert(x)[0]['label'])\n",
        "df_financial_tweets_sentiment_balanceado['confidence'] = df_financial_tweets_sentiment_balanceado['tweet'].apply(lambda x: finbert(x)[0]['score'])\n",
        "\n",
        "# Mostrar resultados\n",
        "print(df_financial_tweets_sentiment_balanceado)\n",
        "df_financial_tweets_sentiment_balanceado.to_excel(\"FTS_balanceado_FinBERT.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "sBGUtOgk4mml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Asegurarse de que las etiquetas estén en el mismo formato (ej. capitalización)\n",
        "df_financial_tweets_sentiment_balanceado[\"sentiment_mapped\"] = df_financial_tweets_sentiment_balanceado[\"sentiment_mapped\"].str.lower().str.strip()\n",
        "df_financial_tweets_sentiment_balanceado[\"FinBERT_sentiment\"] = df_financial_tweets_sentiment_balanceado[\"FinBERT_sentiment\"].str.lower().str.strip()\n",
        "\n",
        "# Calcular matriz de confusión\n",
        "cm = confusion_matrix(df_financial_tweets_sentiment_balanceado[\"sentiment_mapped\"], df_financial_tweets_sentiment_balanceado[\"FinBERT_sentiment\"], labels=[\"positive\", \"negative\", \"neutral\"])\n",
        "\n",
        "# Mostrar como DataFrame\n",
        "cm_df_financial_tweets_sentiment_balanceado = pd.DataFrame(cm, index=[\"True_Positive\", \"True_Negative\", \"True_Neutral\"],\n",
        "                        columns=[\"Pred_Positive\", \"Pred_Negative\", \"Pred_Neutral\"])\n",
        "\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df_financial_tweets_sentiment_balanceado)\n",
        "\n",
        "# Reporte de métricas\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(df_financial_tweets_sentiment_balanceado[\"sentiment_mapped\"], df_financial_tweets_sentiment_balanceado[\"FinBERT_sentiment\"], digits=3))\n",
        "\n",
        "# Visualización con heatmap\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Pred_Positive\", \"Pred_Negative\", \"Pred_Neutral\"],\n",
        "            yticklabels=[\"True_Positive\", \"True_Negative\", \"True_Neutral\"])\n",
        "plt.xlabel(\"PREDICCIONES\")\n",
        "plt.ylabel(\"VALORES REALES\")\n",
        "plt.title(\"Matriz de Confusión (Dataset Tweets - Balanceado)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GA-rsEfH4u6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Modelo DistilRoBERTa + Fine-tuning"
      ],
      "metadata": {
        "id": "1CaQHb4X42bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Cargar el modelo desde Hugging Face\n",
        "model_name = \"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Creamos el pipeline con el modelo\n",
        "sentiment_model = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\",\n",
        "    truncation=True  # distilroberta solo admite secuencias de máximo 512 tokens, pero en tus datos (tweet) hay textos más largos (587 tokens en algún tweet). Eso dispara el RuntimeError en la capa de embeddings y da error en la ejecución. SOLUCIÓN: Truncar cualquier texto más largo al máximo permitido.\n",
        ")\n",
        "\n",
        "# Función para analizar sentimiento\n",
        "def analyze_sentiment(text):\n",
        "    result = sentiment_model(text)[0]\n",
        "    return pd.Series([result['label'], result['score']])\n",
        "\n",
        "# Aplicar análisis a los titulares\n",
        "df_financial_tweets_sentiment_balanceado[['DistilRoBERTa_sentiment', 'confidence']] = df_financial_tweets_sentiment_balanceado['tweet'].apply(analyze_sentiment)\n",
        "\n",
        "print(df_financial_tweets_sentiment_balanceado)\n",
        "df_financial_tweets_sentiment_balanceado.to_excel(\"FTS_balanceado_DistilRoBERTa.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "UJerKzGS40EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Asegurarse de que las etiquetas estén en el mismo formato (ej. capitalización)\n",
        "df_financial_tweets_sentiment_balanceado[\"sentiment_mapped\"] = df_financial_tweets_sentiment_balanceado[\"sentiment_mapped\"].str.lower().str.strip()\n",
        "df_financial_tweets_sentiment_balanceado[\"DistilRoBERTa_sentiment\"] = df_financial_tweets_sentiment_balanceado[\"DistilRoBERTa_sentiment\"].str.lower().str.strip()\n",
        "\n",
        "# Calcular matriz de confusión\n",
        "cm = confusion_matrix(df_financial_tweets_sentiment_balanceado[\"sentiment_mapped\"], df_financial_tweets_sentiment_balanceado[\"DistilRoBERTa_sentiment\"], labels=[\"positive\", \"negative\", \"neutral\"])\n",
        "\n",
        "# Mostrar como DataFrame\n",
        "cm_df_financial_tweets_sentiment_balanceado = pd.DataFrame(cm, index=[\"True_Positive\", \"True_Negative\", \"True_Neutral\"],\n",
        "                        columns=[\"Pred_Positive\", \"Pred_Negative\", \"Pred_Neutral\"])\n",
        "\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df_financial_tweets_sentiment_balanceado)\n",
        "\n",
        "# Reporte de métricas\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(df_financial_tweets_sentiment_balanceado[\"sentiment_mapped\"], df_financial_tweets_sentiment_balanceado[\"DistilRoBERTa_sentiment\"], digits=3))\n",
        "\n",
        "# Visualización con heatmap\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Pred_Positive\", \"Pred_Negative\", \"Pred_Neutral\"],\n",
        "            yticklabels=[\"True_Positive\", \"True_Negative\", \"True_Neutral\"])\n",
        "plt.xlabel(\"PREDICCIONES\")\n",
        "plt.ylabel(\"VALORES REALES\")\n",
        "plt.title(\"Matriz de Confusión (Dataset Tweets - Balanceado)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tNmRjLJm497p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Modelo BERTweet"
      ],
      "metadata": {
        "id": "ntlBNh9p5Cv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar el modelo y tokenizer de BERTweet\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Modelo fine-tuned para análisis de sentimiento\n",
        "MODEL_NAME = \"finiteautomata/bertweet-base-sentiment-analysis\" # Este modelo ya está afinado específicamente para clasificación de sentimiento en tweets\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Creamos una función para predecir sentimiento\n",
        "def predict_sentiment(tweet: str) -> str:\n",
        "    inputs = tokenizer(tweet, return_tensors=\"pt\", truncation=True)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        label_id = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "    labels = model.config.id2label\n",
        "    return labels[label_id]\n",
        "\n",
        "# Lo aplico a mis tweets en formato original (sin preprocesado) porque el modelo de Hugging Face está entrenado para interpretar todo (emojis, menciones, etc.) como señales de sentimiento\n",
        "tqdm.pandas() # Para mostrar una barra de progreso\n",
        "# Convierto los valores a cadenas y reemplazo los NaN por texto vacío\n",
        "df_financial_tweets_sentiment_balanceado['tweet'] = df_financial_tweets_sentiment_balanceado['tweet'].fillna('').astype(str)\n",
        "# Creamos una nueva columna llamada bertweet_sentiment con etiquetas como NEG, NEU, POS llamando a la función anterior\n",
        "df_financial_tweets_sentiment_balanceado['BERTweet_sentiment'] = df_financial_tweets_sentiment_balanceado['tweet'].progress_apply(predict_sentiment)\n",
        "\n",
        "# Guardamos los resultados\n",
        "df_financial_tweets_sentiment_balanceado.to_excel(\"FTS_balanceado_BERTweet.xlsx\", index=False)\n",
        "df_financial_tweets_sentiment_balanceado.head()"
      ],
      "metadata": {
        "id": "0OhwLhvH5LnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tengo que mapear la etiqueta de sentiminto generada por mi modelo\n",
        "def map_labels(label):\n",
        "    if label == \"POS\":\n",
        "        return \"POSITIVE\"\n",
        "    elif label == \"NEG\":\n",
        "        return \"NEGATIVE\"\n",
        "    else:\n",
        "        return \"NEUTRAL\"\n",
        "\n",
        "df_financial_tweets_sentiment_balanceado[\"BERTweet_sentiment_mapped\"] = df_financial_tweets_sentiment_balanceado[\"BERTweet_sentiment\"].apply(map_labels)\n",
        "\n",
        "df_financial_tweets_sentiment_balanceado.head()"
      ],
      "metadata": {
        "id": "1OfsdAWI5W5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Asegurarse de que las etiquetas estén en el mismo formato (ej. capitalización)\n",
        "df_financial_tweets_sentiment_balanceado[\"sentiment_mapped\"] = df_financial_tweets_sentiment_balanceado[\"sentiment_mapped\"].str.lower().str.strip()\n",
        "df_financial_tweets_sentiment_balanceado[\"BERTweet_sentiment_mapped\"] = df_financial_tweets_sentiment_balanceado[\"BERTweet_sentiment_mapped\"].str.lower().str.strip()\n",
        "\n",
        "# Calcular matriz de confusión\n",
        "cm = confusion_matrix(df_financial_tweets_sentiment_balanceado[\"sentiment_mapped\"], df_financial_tweets_sentiment_balanceado[\"BERTweet_sentiment_mapped\"], labels=[\"positive\", \"negative\", \"neutral\"])\n",
        "\n",
        "# Mostrar como DataFrame\n",
        "cm_df_financial_tweets_sentiment_balanceado = pd.DataFrame(cm, index=[\"True_Positive\", \"True_Negative\", \"True_Neutral\"],\n",
        "                        columns=[\"Pred_Positive\", \"Pred_Negative\", \"Pred_Neutral\"])\n",
        "\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_df_financial_tweets_sentiment_balanceado)\n",
        "\n",
        "# Reporte de métricas\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(df_financial_tweets_sentiment_balanceado[\"sentiment_mapped\"], df_financial_tweets_sentiment_balanceado[\"BERTweet_sentiment_mapped\"], digits=3))\n",
        "\n",
        "# Visualización con heatmap\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Pred_Positive\", \"Pred_Negative\", \"Pred_Neutral\"],\n",
        "            yticklabels=[\"True_Positive\", \"True_Negative\", \"True_Neutral\"])\n",
        "plt.xlabel(\"PREDICCIONES\")\n",
        "plt.ylabel(\"VALORES REALES\")\n",
        "plt.title(\"Matriz de Confusión (Dataset Tweets - Balanceado)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RdyDqAKu5Z8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Modelo VADER"
      ],
      "metadata": {
        "id": "2S9J64hO5lB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalamos las librerías necesarias\n",
        "!pip install pyspellchecker\n",
        "!pip install scattertext\n",
        "!pip install nltk\n",
        "!pip install -U kaleido"
      ],
      "metadata": {
        "id": "2jaX3WFS5dAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Data Preprocessing and Wrangling libraries\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import dateutil.parser\n",
        "\n",
        "# Import NLP Libraries\n",
        "import nltk\n",
        "from spellchecker import SpellChecker\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
        "\n",
        "# Import Visualization Libraries\n",
        "import plotly.offline as pyo\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "from plotly.subplots import make_subplots\n",
        "import seaborn as sns\n",
        "import scattertext as st\n",
        "from IPython.display import IFrame\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "import random\n",
        "\n",
        "# Downloading periphrals\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "zTtlCyNj5qtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializamos las herramientas\n",
        "\n",
        "# Para visualizaciones con seaborn\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "# Análisis de sentimiento con VADER\n",
        "sia = SIA()\n",
        "\n",
        "# Corrector ortográfico (puede ayudar antes del análisis de sentimiento)\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Para mostrar gráficos Plotly en notebooks\n",
        "pyo.init_notebook_mode()"
      ],
      "metadata": {
        "id": "LDsUA8sa5s8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Aplicamos pipeline de preprocesamiento para modelo VADER (limpieza más profunda)\"\"\"\n",
        "\n",
        "# Se crea una copia del DataFrame original df_tweets para no modificarlo directamente\n",
        "data = df_financial_tweets_sentiment_balanceado.copy()\n",
        "# Se añade una columna original_tweet con el texto sin procesar a modo de backup para comparar\n",
        "data['original_tweet'] = df_financial_tweets_sentiment_balanceado['tweet']\n",
        "# Reemplaza los valores nulos (NaN) en la columna \"text\" por cadenas vacías '' y convierte todo el contenido de la columna \"text\" a tipo string.\n",
        "# Esto sirve para evitar que el modelo falle al encontrarse con un NaN y para garantizar que todo se maneje como string.\n",
        "data['tweet'] = data['tweet'].fillna('').astype(str)\n",
        "\n",
        "\n",
        "# Aplica la función de limpieza a la columna \"text\"\n",
        "data['tweet'] = data['tweet'].apply(limpieza_vader)\n",
        "# Eliminamos duplicados\n",
        "data.drop_duplicates(subset=[\"tweet\"], inplace=True)\n",
        "# Reseteamos el índice por si se ha eliminado alguna fila\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "wje8nkJF5vkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "# Ancho máximo de cada línea (ejemplo: 80 caracteres)\n",
        "width = 80\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"tweet ANTES de preprocesamiento:\")\n",
        "print(\"=\"*80)\n",
        "print(textwrap.fill(data.original_tweet[0], width=width))  # Aquí hace el salto de línea\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "#print(data.original_tweet[0])\n",
        "print(\"tweet DESPUÉS de preprocesamiento:\")\n",
        "print(80*\"=\")\n",
        "print(textwrap.fill(data.tweet[0], width=width))  # También aquí\n",
        "#print(data.text[0])"
      ],
      "metadata": {
        "id": "vE_wMUrv6F5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esta función convierte un valor de sentimiento (entre -1 y 1, generado por VADER) en una etiqueta categórica\n",
        "def label_sentiment(x:float):\n",
        "    if x < -0.05 : return 'negative'  # negative si es muy bajo\n",
        "    if x > 0.05 : return 'positive'   # positive si es alto\n",
        "    return 'neutral'                  # neutral si está entre ambos umbrales\n",
        "\n",
        "# EXTRACCIÓN DE CARACTERÍSTICAS del texto\n",
        "# Extrae todas las palabras de cada tweet usando regex, eliminando puntuación.\n",
        "data['words'] = data.tweet.apply(lambda x:re.findall(r'\\w+', x ))\n",
        "# Usa SpellChecker para detectar palabras mal escritas\n",
        "data['errors'] = data.words.apply(spell.unknown)\n",
        "# Cuenta cuántos errores ortográficos y cuántas palabras hay por tweet\n",
        "data['errors_count'] = data.errors.apply(len)\n",
        "data['words_count'] = data.words.apply(len)\n",
        "# Longitud del tweet (en caracteres)\n",
        "data['sentence_length'] = data.tweet.apply(len)\n",
        "\n",
        "# Análisis de sentimiento para cada tweet\n",
        "# Aplica VADER (SentimentIntensityAnalyzer = SIA) a cada tweet para obtener el sentimiento compuesto (compound score), que va de -1 (negativo) a 1 (positivo).\n",
        "data['compound'] = [sia.polarity_scores(x)['compound'] for x in tqdm(data['tweet'])] # Se usa tqdm para mostrar una barra de progreso\n",
        "# Clasifica el sentimiento numérico (compound) en positive, neutral o negative con la función definida al inicio.\n",
        "data['VADER_sentiment'] = data['compound'].apply(label_sentiment);\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "ay_iUB5U6Ge3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos los resultados\n",
        "data.to_excel(\"FTS_balanceado_VADER.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "4i-VDbmX6JTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Asegurarse de que las etiquetas estén en el mismo formato (ej. capitalización)\n",
        "data[\"sentiment_mapped\"] = data[\"sentiment_mapped\"].str.lower().str.strip()\n",
        "data[\"VADER_sentiment\"] = data[\"VADER_sentiment\"].str.lower().str.strip()\n",
        "\n",
        "# Calcular matriz de confusión\n",
        "cm = confusion_matrix(data[\"sentiment_mapped\"], data[\"VADER_sentiment\"], labels=[\"positive\", \"negative\", \"neutral\"])\n",
        "\n",
        "# Mostrar como DataFrame\n",
        "cm_data = pd.DataFrame(cm, index=[\"True_Positive\", \"True_Negative\", \"True_Neutral\"],\n",
        "                        columns=[\"Pred_Positive\", \"Pred_Negative\", \"Pred_Neutral\"])\n",
        "\n",
        "print(\"Matriz de confusión:\")\n",
        "print(cm_data)\n",
        "\n",
        "# Reporte de métricas\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(data[\"sentiment_mapped\"], data[\"VADER_sentiment\"], digits=3))\n",
        "\n",
        "# Visualización con heatmap\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Pred_Positive\", \"Pred_Negative\", \"Pred_Neutral\"],\n",
        "            yticklabels=[\"True_Positive\", \"True_Negative\", \"True_Neutral\"])\n",
        "plt.xlabel(\"PREDICCIONES\")\n",
        "plt.ylabel(\"VALORES REALES\")\n",
        "plt.title(\"Matriz de Confusión (Dataset Tweets - Balanceado)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GPCgMCF46Rhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualización comparativa de modelos"
      ],
      "metadata": {
        "id": "4hZm5Kia6maQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Métricas\n",
        "metrics = [\"Accuracy\", \"F1 Negative\", \"F1 Neutral\", \"F1 Pos\", \"F1 Macro\", \"Eficiencia Computacional\"]\n",
        "\n",
        "# ---------- Datos Titulares Original ----------\n",
        "FinBERT_tit =       [0.557, 0.643, 0.577, 0.487, 0.569, 8]    # 8 min\n",
        "DistilRoBERTa_tit = [0.545, 0.626, 0.563, 0.483, 0.558, 2]    # 2 min\n",
        "BERTweet_tit =      [0.503, 0.626, 0.519, 0.427, 0.524, 42]   # 42 min\n",
        "VADER_tit =         [0.430, 0.404, 0.396, 0.471, 0.424, 0.05] # 3 seg ≈ 0.05 min\n",
        "\n",
        "# ---------- Datos Tweets Original ----------\n",
        "FinBERT_tw =       [0.464, 0.498, 0.507, 0.374, 0.460, 11]   # 11 min\n",
        "DistilRoBERTa_tw = [0.502, 0.519, 0.527, 0.456, 0.501, 4]    # 4 min\n",
        "BERTweet_tw =      [0.512, 0.542, 0.522, 0.483, 0.516, 73]   # 73 min\n",
        "VADER_tw =         [0.451, 0.436, 0.420, 0.487, 0.448, 0.12] # 7 seg ≈ 0.12 min\n",
        "\n",
        "# Normalizar e invertir coste → eficiencia\n",
        "# Extraer todos los tiempos\n",
        "time_values = [\n",
        "    FinBERT_tit[-1], DistilRoBERTa_tit[-1], BERTweet_tit[-1], VADER_tit[-1],\n",
        "    FinBERT_tw[-1], DistilRoBERTa_tw[-1], BERTweet_tw[-1], VADER_tw[-1]\n",
        "]\n",
        "\n",
        "# Normalización logarítmica + min-max\n",
        "log_times = [np.log(t + 1) for t in time_values]\n",
        "min_log, max_log = min(log_times), max(log_times)\n",
        "\n",
        "for model in [FinBERT_tit, DistilRoBERTa_tit, BERTweet_tit, VADER_tit,\n",
        "              FinBERT_tw, DistilRoBERTa_tw, BERTweet_tw, VADER_tw]:\n",
        "    model[-1] = ((max_log - np.log(model[-1] + 1)) / (max_log - min_log)) + 0.1\n",
        "\n",
        "\n",
        "# Ángulos radar\n",
        "angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
        "angles += angles[:1]\n",
        "\n",
        "def close_values(values):\n",
        "    return values + values[:1]\n",
        "\n",
        "# Función para dibujar cada radar\n",
        "def plot_radar(ax, title, models):\n",
        "    ax.set_xticks(angles[:-1])\n",
        "    ax.set_xticklabels(metrics)\n",
        "    ax.set_title(title, size=18, y=1.08)\n",
        "\n",
        "    for name, values in models.items():\n",
        "        vals = close_values(values)\n",
        "        ax.plot(angles, vals, label=name)\n",
        "        ax.fill(angles, vals, alpha=0.1)\n",
        "\n",
        "# Crear la figura con dos subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7), subplot_kw=dict(polar=True))\n",
        "\n",
        "# Radar Titulares Original\n",
        "plot_radar(ax1, \"News_O\", {\n",
        "    \"FinBERT\": FinBERT_tit,\n",
        "    \"DistilRoBERTa\": DistilRoBERTa_tit,\n",
        "    \"BERTweet\": BERTweet_tit,\n",
        "    \"VADER\": VADER_tit\n",
        "})\n",
        "\n",
        "# Radar Tweets Original\n",
        "plot_radar(ax2, \"Tweet_O\", {\n",
        "    \"FinBERT\": FinBERT_tw,\n",
        "    \"DistilRoBERTa\": DistilRoBERTa_tw,\n",
        "    \"BERTweet\": BERTweet_tw,\n",
        "    \"VADER\": VADER_tw\n",
        "})\n",
        "\n",
        "# Eliminar duplicados en la leyenda\n",
        "handles, labels = fig.axes[0].get_legend_handles_labels()\n",
        "by_label = dict(zip(labels, handles))\n",
        "fig.legend(by_label.values(), by_label.keys(), loc=\"upper center\", bbox_to_anchor=(0.5, 1.1), ncol=1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "295EkKh662Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recolección de datos desde diferentes fuentes"
      ],
      "metadata": {
        "id": "ZMETxEQlh9m3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. PyGoogleNews para obtención de titulares de noticias"
      ],
      "metadata": {
        "id": "CLKUl1PniOo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install feedparser\n",
        "!pip install dateparser\n",
        "!pip install plotly\n",
        "import pygooglenews   # Importo mi módulo pygooglenews.py"
      ],
      "metadata": {
        "id": "G9uhHHNji1QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pygooglenews import GoogleNews  # Importo la clase GoogleNews de mi módulo pygooglenews.py\n",
        "import datetime  # Para manejo de fechas\n",
        "import time  # Para pausar entre solicitudes\n",
        "\n",
        "# Crea un objeto buscador de noticias para consultar noticias desde Google News\n",
        "gn = GoogleNews()\n",
        "\n",
        "# Esta función busca noticias relacionadas con un término (por ejemplo: \"Tesla OR TSLA\")\n",
        "# entre una determinada fecha, dividiendo la búsqueda en bloques de 30 días para evitar límites\n",
        "# en la cantidad de resultados que Google News devuelve de una sola vez.\n",
        "def get_news(search):\n",
        "  stories = []  # Lista\n",
        "\n",
        "  start_date = datetime.date(2025,8,27)\n",
        "  end_date = datetime.date(2025,8,29)\n",
        "  delta = datetime.timedelta(days=1) # Con esta línea recorro el año día a día para mejorar la precisión diaria, realizando 365 consultas a Google News (OJO con los bloqueos por el exceso de peticiones).\n",
        "  date = start_date\n",
        "\n",
        "  while date <= end_date:\n",
        "        try:\n",
        "            print(f\"Buscando noticias del {date}...\")\n",
        "            result = gn.search(\n",
        "                search,\n",
        "                from_=date.strftime('%Y-%m-%d'),\n",
        "                to_=(date + delta).strftime('%Y-%m-%d')\n",
        "            )\n",
        "\n",
        "            newsitems = result['entries']\n",
        "\n",
        "            for item in newsitems:\n",
        "                story = {\n",
        "                    'title': item.title,\n",
        "                    'published': datetime.datetime.strptime(item.published, '%a, %d %b %Y %X GMT')\n",
        "                }\n",
        "                stories.append(story)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error en {date}: {e}\")\n",
        "\n",
        "        # Esperar 2 segundos antes de la siguiente petición\n",
        "        time.sleep(2)\n",
        "        date += delta\n",
        "\n",
        "  return stories\n",
        "\n",
        "# Convertimos la lista de diccionarios stories en un DataFrame,\n",
        "# a la par que llamamos a la función get_news con el “[Nombre Compañia] OR [Ticker]” como parámetro.\n",
        "# Ticker = etiqueta de cotización con el que opera en bolsa.\n",
        "df = pd.DataFrame(get_news('Amazon OR AMZN'))\n",
        "\n",
        "# Agregar columna con solo la fecha\n",
        "df['date'] = df['published'].dt.date\n",
        "\n",
        "# Mostrar cantidad de noticias por día\n",
        "news_per_day = df.groupby('date').size()\n",
        "print(news_per_day)"
      ],
      "metadata": {
        "id": "1zNFqrY0i2HY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se guardan los resultados de las noticias recolectadas\n",
        "df.to_excel(\"noticias_PyGoogleNews.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "2ao29EHUj_UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estas líneas es por si queremos filtrar un día concreto de noticias\n",
        "df['date'] = df['published'].dt.date\n",
        "df = df[df['date'] == datetime.date(2025, 8, 28)]"
      ],
      "metadata": {
        "id": "zw5PHyP4jKyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "lqJJ-0lMjXFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "-QqjPlRNjdbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Preparar los datos\n",
        "df['date'] = df['published'].dt.date\n",
        "news_per_day = df.groupby('date').size().reset_index()\n",
        "news_per_day.columns = ['date', 'count']\n",
        "news_per_day['rolling_7'] = news_per_day['count'].rolling(window=7).mean()\n",
        "\n",
        "# Crear figura\n",
        "fig = go.Figure()\n",
        "\n",
        "# Línea principal (noticias por día)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=news_per_day['date'],\n",
        "    y=news_per_day['count'],\n",
        "    mode='lines',\n",
        "    name='Noticias por día',\n",
        "    line=dict(color='royalblue')\n",
        "))\n",
        "\n",
        "# Media móvil de 7 días\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=news_per_day['date'],\n",
        "    y=news_per_day['rolling_7'],\n",
        "    mode='lines',\n",
        "    name='Media móvil (7 días)',\n",
        "    line=dict(color='orange', dash='dash')\n",
        "))\n",
        "\n",
        "# Picos de noticias (top 5 días con más noticias)\n",
        "top_days = news_per_day.nlargest(5, 'count')\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=top_days['date'],\n",
        "    y=top_days['count'],\n",
        "    mode='markers+text',\n",
        "    name='Picos de noticias',\n",
        "    marker=dict(color='red', size=10),\n",
        "    text=top_days['date'].astype(str),\n",
        "    textposition='top center'\n",
        "))\n",
        "\n",
        "# =====================\n",
        "# 🔹 Anotaciones de eventos relevantes (ejemplo: resultados financieros)\n",
        "# =====================\n",
        "eventos = {\n",
        "    \"2025-02-26\": \"Resultados Q4 2024\",\n",
        "    \"2025-05-28\": \"Resultados Q1 2025\",\n",
        "    #\"2025-07-23\": \"Resultados Q2 2025\",\n",
        "    #\"2024-10-23\": \"Resultados Q3 2024\",\n",
        "    \"2025-01-27\": \"Caída del 17% por noticia DeepSeek\",\n",
        "    \"2025-03-18\": \"Conferencia anual GPU Technology Conference\",\n",
        "    #\"2025-06-06\": \"Enfrentamiento público entre Elon Musk y Trump\",\n",
        "    #\"2025-07-07\": \"Anuncio de Elon Musk sobre la creación del nuevo partido político llamado America Party\"\n",
        "}\n",
        "for fecha, texto in eventos.items():\n",
        "    fig.add_annotation(\n",
        "        x=fecha,\n",
        "        y=news_per_day.loc[news_per_day['date'] == pd.to_datetime(fecha).date(), 'count'].values[0],\n",
        "        text=texto,\n",
        "        showarrow=True,\n",
        "        arrowhead=2,\n",
        "        ax=40, ay=-40,\n",
        "        bgcolor=\"white\"\n",
        "    )\n",
        "\n",
        "\"\"\"# =====================\n",
        "# 🔹 Sombreado de un periodo de interés (ejemplo: semana de alta cobertura)\n",
        "# =====================\n",
        "fig.add_vrect(\n",
        "    x0=\"2024-05-10\", x1=\"2024-05-20\",\n",
        "    fillcolor=\"lightgrey\", opacity=0.3,\n",
        "    layer=\"below\", line_width=0,\n",
        "    annotation_text=\"Alta cobertura\",\n",
        "    annotation_position=\"top left\"\n",
        ")\"\"\"\n",
        "\n",
        "# Estética del gráfico\n",
        "fig.update_layout(\n",
        "    title='Cobertura diaria de noticias sobre NVIDIA (2025)',\n",
        "    xaxis_title='Fecha',\n",
        "    yaxis_title='Cantidad de noticias',\n",
        "    hovermode='x unified',\n",
        "    template='plotly_white',\n",
        "    width=1000,\n",
        "    height=500,\n",
        "    legend=dict(\n",
        "        orientation=\"h\",\n",
        "        yanchor=\"bottom\", y=-0.3,\n",
        "        xanchor=\"center\", x=0.5\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "kZSK5BFBje7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Financial Modeling Prep (FMP) para obtención de titulares de últimas noticias"
      ],
      "metadata": {
        "id": "pwQHNFGmkMRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FMP es la abreviatura de Financial Modeling Prep, una plataforma que ofrece una API financiera robusta y flexible para acceder a datos bursátiles y económicos en tiempo real e históricos. Es ampliamente utilizada por desarrolladores, analistas financieros, investigadores y estudiantes para integrar información financiera en aplicaciones, hojas de cálculo y modelos de análisis.\n",
        "\n",
        "Su API en versión gratuita solo permite:\n",
        "\n",
        "\n",
        "*   Últimas 200 noticias financieras de las empresas que sean.\n",
        "*   NO permite filtrar por empresa o por fecha.\n",
        "*   Variedad de endpoints para poder hacer distintas peticiones (no solo noticias), como datos técnicos financieros, datos de empresa específica, etc. --> Ver todo los tipos en \"https://site.financialmodelingprep.com/developer/docs/stable\""
      ],
      "metadata": {
        "id": "E0m-7DOvkYmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "API_KEY = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXX'  # Reemplaza con tu API key real\n",
        "tickers = 'TSLA'   # Al buscar noticias en la versión free no me permite filtrar por empresa\n",
        "limit = 500  # El límite máximo de últimas noticias que me devuelve la API versión gratuita es 200\n",
        "\n",
        "# Existen mucha variedad de endpoints: datos financieros, datos de empresa específica, noticias...\n",
        "# La URL siguiente hace referencia al único endpoint de noticias que me permite la versión gratis (200 noticias máximo)\n",
        "url = f'https://financialmodelingprep.com/stable/fmp-articles?page=0&limit={limit}&apikey={API_KEY}'\n",
        "\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "\n",
        "    # Convertir a DataFrame\n",
        "    df_news = pd.DataFrame(data)\n",
        "    print(df_news)\n",
        "else:\n",
        "    print(\"Error:\", response.status_code, response.text)"
      ],
      "metadata": {
        "id": "4bK_UoxWkDKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos las noticias recientes extraídas\n",
        "df_news.to_excel(\"ultimas_noticias_FMP.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "KblpVk8LkyfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news.head()"
      ],
      "metadata": {
        "id": "jNHYhgQuk9uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtro los 200 artículos por el ticker deseado\n",
        "TICKER = 'TSLA'  # Ticker que quiera filtrar\n",
        "FECHA_MINIMA = '2025-08-04'  # Fecha (incluida) a partir de la cual quiero obtener las noticias\n",
        "\n",
        "# Convierto la columna 'date' a formato datetime\n",
        "df_news['date'] = pd.to_datetime(df_news['date'])\n",
        "\n",
        "# Filtro si la columna 'tickers' contiene ese símbolo. También filtro por fecha\n",
        "df_FMP = df_news[\n",
        "    (df_news['tickers'].apply(lambda x: TICKER in x if isinstance(x, list) else TICKER in str(x))) &\n",
        "    (df_news['date'] >= pd.to_datetime(FECHA_MINIMA))\n",
        "]\n",
        "\n",
        "df_FMP.head()"
      ],
      "metadata": {
        "id": "xvDrQpDak-It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. YFinance para obtención de datos financieros"
      ],
      "metadata": {
        "id": "R4kn7YW0lMJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance"
      ],
      "metadata": {
        "id": "U3j1RSJJlGGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "\n",
        "df_precio = yf.download('AMZN', start = '2025-08-27', end = '2025-08-29', group_by='ticker')\n",
        "\n",
        "# yfinance devuelve por defecto columnas anidadas (MultiIndex en columnas). Hay que realizar los siguientes pasos para eliminar el MultiIndex y obtener un dataframe normal\n",
        "# 1. Resetear índice para tener 'Date' como columna\n",
        "df_precio.reset_index(inplace=True)\n",
        "\n",
        "# 2. Aplanar columnas (eliminar MultiIndex)\n",
        "df_precio.columns = [col[1] if isinstance(col, tuple) else col for col in df_precio.columns]\n",
        "\n",
        "# 3. Renombrar columna de fecha (si no se llama 'Date')\n",
        "if 'Date' not in df_precio.columns:\n",
        "    df_precio.rename(columns={df_precio.columns[0]: 'Date'}, inplace=True)\n",
        "\n",
        "df_precio.head()"
      ],
      "metadata": {
        "id": "SxsLYrWclSdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_precio.shape"
      ],
      "metadata": {
        "id": "TCq8SKsIlXaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_precio.info()"
      ],
      "metadata": {
        "id": "mIIgcC77lZ0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. API Twitter/X para obtención de tweets y captar ruido social"
      ],
      "metadata": {
        "id": "aUXKljvLljfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Tweets relacionados con alguna compañía o activo financiero"
      ],
      "metadata": {
        "id": "T5-0_hjhmXQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweepy # Biblioteca de Python para interactuar con la API de Twitter"
      ],
      "metadata": {
        "id": "ytdDHejjlbBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "\n",
        "# Pega aquí tu Bearer Token\n",
        "BEARER_TOKEN = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
        "\n",
        "# Conexión con el cliente Tweepy\n",
        "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
        "\n",
        "# Define la consulta: por ejemplo, tweets que mencionen \"Tesla\", en inglés, excluyendo retweets\n",
        "query = 'Tesla lang:en -is:retweet'\n",
        "\n",
        "# Búsqueda de hasta 10 tweets recientes\n",
        "tweets = client.search_recent_tweets(query=query, max_results=10)\n",
        "\n",
        "# Mostrar texto de tweets\n",
        "if tweets.data:\n",
        "    for tweet in tweets.data:\n",
        "        print(tweet.text)\n",
        "else:\n",
        "    print(\"No se encontraron tweets\")"
      ],
      "metadata": {
        "id": "CixMsvLommrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Tweets recientes publicados por un usuario específico"
      ],
      "metadata": {
        "id": "glczJKTSm5WE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "\n",
        "# Tus credenciales de la API de Twitter/X\n",
        "BEARER_TOKEN = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
        "\n",
        "# Configurar cliente de tweepy con autenticación Bearer Token\n",
        "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
        "\n",
        "# Nombre de usuario sin '@'\n",
        "username = \"elonmusk\"\n",
        "\n",
        "# Obtener user ID del usuario\n",
        "user = client.get_user(username=username)\n",
        "user_id = user.data.id\n",
        "\n",
        "# Obtener tweets recientes del usuario (máximo 10)\n",
        "tweets = client.get_users_tweets(id=user_id, max_results=10)\n",
        "\n",
        "# Imprimir texto y fecha de los tweets\n",
        "for tweet in tweets.data:\n",
        "    print(tweet.created_at, \"-\", tweet.text)"
      ],
      "metadata": {
        "id": "-6OS7XxWm9oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Reddit para obtención de posts como fuente alternativa de sentimiento minorista"
      ],
      "metadata": {
        "id": "PCgdBZfun5y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw  # Biblioteca de Python para interactuar con la API de Reddit"
      ],
      "metadata": {
        "id": "VPdIAtB-oNQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import datetime\n",
        "\n",
        "# Configura tu app en https://www.reddit.com/prefs/apps para obtener client_id, client_secret y user_agent\n",
        "reddit = praw.Reddit(\n",
        "    client_id='XXXXXXXXXXXXXXXXXXXXXXXX',\n",
        "    client_secret='XXXXXXXXXXXXXXXXXXXXXXXX',\n",
        "    user_agent='XXXXXXXXXXXXXXXXXXXXXXXX'  # Es un identificador para que Reddit sepa quién hace la petición, puede ser cualquier texto descriptivo.\n",
        ")\n",
        "\n",
        "# Elige un subreddit, por ejemplo r/WallStreetBets o r/investing\n",
        "subreddit = reddit.subreddit('wallstreetbets')\n",
        "\n",
        "# Lista para almacenar los datos\n",
        "posts_data = []\n",
        "\n",
        "# Obtener los 500 posts más recientes --> Devuelve como máximo los 1.000 aproximadamente más recientes\n",
        "for post in subreddit.new(limit=500):\n",
        "    posts_data.append({\n",
        "        'title': post.title,\n",
        "        'text': post.selftext,\n",
        "        'created': datetime.datetime.fromtimestamp(post.created_utc),\n",
        "        'url': post.url\n",
        "    })\n",
        "\n",
        "# Convertimos a DataFrame\n",
        "df_reddit = pd.DataFrame(posts_data)\n",
        "\n",
        "# Mostramos los primeros resultados\n",
        "print(df_reddit.head())"
      ],
      "metadata": {
        "id": "kzFvG4OtoRlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_reddit.head()"
      ],
      "metadata": {
        "id": "y5D-flSnox9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_reddit.shape"
      ],
      "metadata": {
        "id": "IKk201UHozoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultados y Estudios"
      ],
      "metadata": {
        "id": "jWOjMdy8rP2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.\tTitulares de noticias por compañía y periodo específico vs Precio de acciones"
      ],
      "metadata": {
        "id": "VtltJ7bdrfRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la obtención de datos seleccionamos previamente:\n",
        "\n",
        "1.   Una compañía específica.\n",
        "2.   Un rango de fechas específico."
      ],
      "metadata": {
        "id": "SZ-EM-d3ruYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Asignamos un valor numérico al sentimiento\n",
        "sentiment_map = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
        "df['sentiment_score'] = df['sentiment'].map(sentiment_map)\n",
        "\n",
        "# Agrupamos por fecha (media diaria del sentimiento)\n",
        "df_sentiment_daily = df.groupby('date')['sentiment_score'].mean().reset_index()\n",
        "\n",
        "# Convertimos la columna \"published\", que es de tipo objeto a tipo datetime\n",
        "df_sentiment_daily['date'] = pd.to_datetime(df_sentiment_daily['date'])\n",
        "\n",
        "df_sentiment_daily.head()"
      ],
      "metadata": {
        "id": "vMDijnmhrrAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JOIN: Unimos sentimiento con precios por fecha\n",
        "df_merged = pd.merge(df_precio, df_sentiment_daily, left_on='Date', right_on='date', how='left')\n",
        "\n",
        "# Rellenar días sin noticias con 0 (neutral)\n",
        "df_merged['sentiment_score'] = df_merged['sentiment_score'].fillna(0)\n",
        "\n",
        "# Crear la columna de retorno diario\n",
        "df_merged['daily_return'] = df_merged['Close'].pct_change()\n",
        "\n",
        "df_merged.head()"
      ],
      "metadata": {
        "id": "xm-MzHMssCwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sentiment_daily.shape"
      ],
      "metadata": {
        "id": "DbPpaD-bsJYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.shape"
      ],
      "metadata": {
        "id": "q1o2yogrsLTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Correlación numérica\n",
        "correlation = df_merged[['sentiment_score', 'daily_return']].corr()\n",
        "print(correlation)"
      ],
      "metadata": {
        "id": "GKNXBVnxsNxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. La correlación con desfase de 1 día es muy útil porque mide si el sentimiento de hoy tiene algún poder predictivo sobre el retorno de mañana.\n",
        "\n",
        "# ───────────────────────────\n",
        "# Crear columna con el retorno del día siguiente (desfase de 1 día)\n",
        "# ───────────────────────────\n",
        "df_merged['daily_return_lag1'] = df_merged['daily_return'].shift(-1)\n",
        "\n",
        "# ───────────────────────────\n",
        "# Calcular correlación de Pearson entre sentimiento actual y retorno del día siguiente\n",
        "# ───────────────────────────\n",
        "corr_lag1 = df_merged[['sentiment_score', 'daily_return_lag1']].corr().iloc[0,1]\n",
        "\n",
        "print(f\"Correlación (sentimiento_t vs. retorno_t+1): {corr_lag1:.4f}\")"
      ],
      "metadata": {
        "id": "mqTEP5nWsS-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Gráfico de dispersión (scatter plot)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df_merged, x='sentiment_score', y='daily_return')\n",
        "plt.title('Correlación entre sentimiento de noticias y retorno diario de TSLA')\n",
        "plt.xlabel('Sentimiento promedio (DistilRoBERTa)')\n",
        "plt.ylabel('Retorno diario (%)')\n",
        "plt.axhline(0, color='gray', linestyle='--')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R9m8zBANsX6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Gráfico combinado en el tiempo\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Copia del dataframe para no modificar el original\n",
        "df_plot = df_merged.copy()\n",
        "\n",
        "# Media móvil de 7 días para suavizar el sentimiento\n",
        "df_plot['sentiment_rolling'] = df_plot['sentiment_score'].rolling(7).mean()\n",
        "\n",
        "# Crear figura y ejes\n",
        "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
        "\n",
        "# --- Eje 1: Precio de cierre ---\n",
        "ax1.plot(df_plot['Date'], df_plot['Close'], color='blue', label='Precio de cierre')\n",
        "ax1.set_ylabel('Precio de acción (USD)', color='blue')\n",
        "ax1.tick_params(axis='y', labelcolor='blue')\n",
        "\n",
        "# --- Eje 2: Sentimiento ---\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "# Dibujar el sentimiento diario con colores (verde si >0, rojo si <0)\n",
        "colors = np.where(df_plot['sentiment_score'] >= 0, 'green', 'red')\n",
        "ax2.bar(df_plot['Date'], df_plot['sentiment_score'],\n",
        "        color=colors, alpha=0.3, width=1.0)\n",
        "\n",
        "# Añadir línea suavizada (media móvil 7 días)\n",
        "ax2.plot(df_plot['Date'], df_plot['sentiment_rolling'],\n",
        "         color='orange', linestyle='--', linewidth=2, label='Sentimiento (media móvil 7 días)')\n",
        "\n",
        "ax2.set_ylabel('Sentimiento promedio (sentiment_score)', color='red')\n",
        "ax2.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "# --- Leyenda manual para diferenciar positivo y negativo ---\n",
        "pos_patch = mpatches.Patch(color='green', alpha=0.3, label='Sentimiento positivo')\n",
        "neg_patch = mpatches.Patch(color='red', alpha=0.3, label='Sentimiento negativo')\n",
        "\n",
        "# --- Título y estética ---\n",
        "plt.title('Evolución del precio de NVDA y sentimiento de titulares de noticias (2025)', fontsize=14, fontweight='bold')\n",
        "fig.tight_layout()\n",
        "\n",
        "# Leyendas combinadas\n",
        "lines_labels = [ax.get_legend_handles_labels() for ax in [ax1, ax2]]\n",
        "lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
        "# Añadir parches personalizados\n",
        "lines += [pos_patch, neg_patch]\n",
        "labels += [pos_patch.get_label(), neg_patch.get_label()]\n",
        "\n",
        "ax1.legend(lines, labels, loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5rJQIMx-saxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Agrupación semanal para mostrar cómo evoluciona el sentimiento semanal frente al precio semanal\n",
        "\n",
        "# Agrupar semanalmente promediando valores numéricos\n",
        "df_weekly = df_merged.set_index('Date').resample('W').mean(numeric_only=True).reset_index()\n",
        "\n",
        "# Calcular correlación semanal\n",
        "correlacion_semanal = df_weekly[['sentiment_score', 'daily_return']].corr().iloc[0, 1]\n",
        "print(f\"📅 Correlación semanal entre sentimiento y retorno: {correlacion_semanal:.4f}\")"
      ],
      "metadata": {
        "id": "qryaAIhiskI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Vamos a comprobar si el sentimiento de la semana anterior predice el retorno de la semana actual.\n",
        "# Esto es muy útil para evaluar si el sentimiento anticipa movimientos de precio.\n",
        "# Para ello aplicamos un desfase (lag).\n",
        "\n",
        "# Crear una nueva columna con el sentimiento de la semana anterior\n",
        "df_weekly['sentiment_lag1'] = df_weekly['sentiment_score'].shift(1)\n",
        "\n",
        "# Calcular correlación entre sentimiento lag y retorno actual\n",
        "correlacion_lag1 = df_weekly[['sentiment_lag1', 'daily_return']].corr().iloc[0, 1]\n",
        "print(f\"📊 Correlación con desfase 1 semana: {correlacion_lag1:.4f}\")"
      ],
      "metadata": {
        "id": "OpEaSU0-stlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Análisis comparativo MISMO DÍA\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Asegurarse de que el DataFrame esté ordenado por fecha\n",
        "df_merged = df_merged.sort_values(by='Date')\n",
        "\n",
        "# ────────────────\n",
        "# Filtrar días por sentimiento y dirección del precio\n",
        "# ────────────────\n",
        "positivos = df_merged[df_merged['sentiment_score'] > 0]\n",
        "negativos = df_merged[df_merged['sentiment_score'] < 0]\n",
        "\n",
        "subidas_positivas = positivos[positivos['daily_return'] > 0]\n",
        "bajadas_negativas = negativos[negativos['daily_return'] < 0]\n",
        "\n",
        "# ────────────────\n",
        "# Calcular porcentajes de coincidencia\n",
        "# ────────────────\n",
        "porcentaje_pos = len(subidas_positivas) / len(positivos) * 100 if len(positivos) > 0 else 0\n",
        "porcentaje_neg = len(bajadas_negativas) / len(negativos) * 100 if len(negativos) > 0 else 0\n",
        "\n",
        "# ────────────────\n",
        "# Crear tabla de resultados\n",
        "# ────────────────\n",
        "tabla_resultados = pd.DataFrame({\n",
        "    'Sentimiento': ['Positivo', 'Negativo'],\n",
        "    'Días totales': [len(positivos), len(negativos)],\n",
        "    'Mov. esperado': [len(subidas_positivas), len(bajadas_negativas)],\n",
        "    '% coincidencia': [porcentaje_pos, porcentaje_neg]\n",
        "})\n",
        "\n",
        "print(tabla_resultados)\n",
        "\n",
        "# ────────────────\n",
        "# Gráfico de barras visual\n",
        "# ────────────────\n",
        "plt.figure(figsize=(8,5))\n",
        "colores = ['green', 'red']\n",
        "plt.bar(tabla_resultados['Sentimiento'], tabla_resultados['% coincidencia'], color=colores, alpha=0.7)\n",
        "plt.ylabel('% días en que el precio se movió según el sentimiento')\n",
        "plt.title('Relación entre sentimiento de noticias y movimiento del precio')\n",
        "plt.ylim(0, 100)\n",
        "\n",
        "# Añadir etiquetas de porcentaje encima de las barras\n",
        "for i, val in enumerate(tabla_resultados['% coincidencia']):\n",
        "    plt.text(i, val + 2, f\"{val:.1f}%\", ha='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5BGyJJ6_sx6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Análisis comparativo DÍA SIGUIENTE\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Asegurarse de que el DataFrame esté ordenado por fecha\n",
        "df_merged = df_merged.sort_values(by='Date')\n",
        "\n",
        "# ────────────────\n",
        "# Desfase de 1 día: el sentimiento de hoy frente al retorno de mañana\n",
        "# ────────────────\n",
        "df_merged['daily_return_next'] = df_merged['daily_return'].shift(-1)\n",
        "\n",
        "# Filtrar días por sentimiento y dirección del precio al día siguiente\n",
        "positivos = df_merged[df_merged['sentiment_score'] > 0]\n",
        "negativos = df_merged[df_merged['sentiment_score'] < 0]\n",
        "\n",
        "subidas_positivas = positivos[positivos['daily_return_next'] > 0]\n",
        "bajadas_negativas = negativos[negativos['daily_return_next'] < 0]\n",
        "\n",
        "# ────────────────\n",
        "# Calcular porcentajes de coincidencia\n",
        "# ────────────────\n",
        "porcentaje_pos = len(subidas_positivas) / len(positivos) * 100 if len(positivos) > 0 else 0\n",
        "porcentaje_neg = len(bajadas_negativas) / len(negativos) * 100 if len(negativos) > 0 else 0\n",
        "\n",
        "# ────────────────\n",
        "# Crear tabla de resultados\n",
        "# ────────────────\n",
        "tabla_resultados = pd.DataFrame({\n",
        "    'Sentimiento': ['Positivo', 'Negativo'],\n",
        "    'Días totales': [len(positivos), len(negativos)],\n",
        "    'Mov. esperado': [len(subidas_positivas), len(bajadas_negativas)],\n",
        "    '% coincidencia': [porcentaje_pos, porcentaje_neg]\n",
        "})\n",
        "\n",
        "print(tabla_resultados)\n",
        "\n",
        "# ────────────────\n",
        "# Gráfico de barras visual\n",
        "# ────────────────\n",
        "plt.figure(figsize=(8,5))\n",
        "colores = ['green', 'red']\n",
        "plt.bar(tabla_resultados['Sentimiento'], tabla_resultados['% coincidencia'], color=colores, alpha=0.7)\n",
        "plt.ylabel('% días en que el precio se movió según el sentimiento (día siguiente)')\n",
        "plt.title('Relación entre sentimiento de noticias y movimiento del precio al día siguiente')\n",
        "plt.ylim(0, 100)\n",
        "\n",
        "# Añadir etiquetas de porcentaje encima de las barras\n",
        "for i, val in enumerate(tabla_resultados['% coincidencia']):\n",
        "    plt.text(i, val + 2, f\"{val:.1f}%\", ha='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FgZw1CJftT2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Análisis comparativo SENTIMIENTOS EXTREMOS (> 0.2 ó < -0.2)\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Asegurarse de que el DataFrame esté ordenado por fecha\n",
        "df_merged = df_merged.sort_values(by='Date')\n",
        "\n",
        "# ────────────────\n",
        "# Desfase de 1 día\n",
        "# ────────────────\n",
        "df_merged['daily_return_next'] = df_merged['daily_return'].shift(-1)\n",
        "\n",
        "# ────────────────\n",
        "# Función para calcular coincidencia según umbrales de sentimiento extremo\n",
        "# ────────────────\n",
        "def porcentaje_coincidencia_extremo(df, sentiment_col, return_col, umbral_pos=0.2, umbral_neg=-0.2):\n",
        "    positivos = df[df[sentiment_col] > umbral_pos]\n",
        "    negativos = df[df[sentiment_col] < umbral_neg]\n",
        "\n",
        "    subidas_positivas = positivos[positivos[return_col] > 0]\n",
        "    bajadas_negativas = negativos[negativos[return_col] < 0]\n",
        "\n",
        "    porcentaje_pos = len(subidas_positivas) / len(positivos) * 100 if len(positivos) > 0 else 0\n",
        "    porcentaje_neg = len(bajadas_negativas) / len(negativos) * 100 if len(negativos) > 0 else 0\n",
        "\n",
        "    tabla = pd.DataFrame({\n",
        "        'Sentimiento': ['Muy positivo', 'Muy negativo'],\n",
        "        'Días totales': [len(positivos), len(negativos)],\n",
        "        'Mov. esperado': [len(subidas_positivas), len(bajadas_negativas)],\n",
        "        '% coincidencia': [porcentaje_pos, porcentaje_neg]\n",
        "    })\n",
        "    return tabla\n",
        "\n",
        "# ────────────────\n",
        "# 1. Coincidencia mismo día\n",
        "# ────────────────\n",
        "tabla_mismo_dia_extremo = porcentaje_coincidencia_extremo(df_merged, 'sentiment_score', 'daily_return')\n",
        "tabla_mismo_dia_extremo['Escenario'] = 'Mismo día'\n",
        "\n",
        "# ────────────────\n",
        "# 2. Coincidencia día siguiente\n",
        "# ────────────────\n",
        "tabla_dia_siguiente_extremo = porcentaje_coincidencia_extremo(df_merged, 'sentiment_score', 'daily_return_next')\n",
        "tabla_dia_siguiente_extremo['Escenario'] = 'Día siguiente'\n",
        "\n",
        "# ────────────────\n",
        "# Combinar tablas\n",
        "# ────────────────\n",
        "tabla_completa_extremo = pd.concat([tabla_mismo_dia_extremo, tabla_dia_siguiente_extremo], ignore_index=True)\n",
        "print(tabla_completa_extremo)\n",
        "\n",
        "# ────────────────\n",
        "# Gráfico comparativo\n",
        "# ────────────────\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "anchura = 0.35\n",
        "x = range(len(tabla_mismo_dia_extremo))\n",
        "\n",
        "# Barras mismo día\n",
        "plt.bar([i - anchura/2 for i in x],\n",
        "        tabla_mismo_dia_extremo['% coincidencia'],\n",
        "        width=anchura,\n",
        "        color=['green','red'],\n",
        "        alpha=0.7,\n",
        "        label='Mismo día')\n",
        "\n",
        "# Barras día siguiente\n",
        "plt.bar([i + anchura/2 for i in x],\n",
        "        tabla_dia_siguiente_extremo['% coincidencia'],\n",
        "        width=anchura,\n",
        "        color=['darkgreen','darkred'],\n",
        "        alpha=0.7,\n",
        "        label='Día siguiente')\n",
        "\n",
        "plt.xticks(ticks=x, labels=tabla_mismo_dia_extremo['Sentimiento'])\n",
        "plt.ylabel('% días en que el precio se movió según el sentimiento extremo')\n",
        "plt.title('Relación entre sentimientos extremos y movimiento del precio de las acciones')\n",
        "plt.ylim(0, 100)\n",
        "plt.legend()\n",
        "\n",
        "# Añadir etiquetas de porcentaje encima de las barras\n",
        "for i, val in enumerate(tabla_mismo_dia_extremo['% coincidencia']):\n",
        "    plt.text(i - anchura/2, val + 2, f\"{val:.1f}%\", ha='center', fontsize=10)\n",
        "for i, val in enumerate(tabla_dia_siguiente_extremo['% coincidencia']):\n",
        "    plt.text(i + anchura/2, val + 2, f\"{val:.1f}%\", ha='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z3EN5VwBtbW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.\tTweets con carácter financiero por compañía y periodo específico vs Precio de acciones"
      ],
      "metadata": {
        "id": "-CP0OlGJuUE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargo el dataset con los tweets recolectados sobre Tesla y periodo del 01-01-2022 a 01-10-2022\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('Tweets_Recolectados_Tesla.xlsx')"
      ],
      "metadata": {
        "id": "lnn5qIoqvCbI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analizamos el sentimiento con el modelo BERTweet, pero podríamos haber usado cualquier otro\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Modelo fine-tuned para análisis de sentimiento\n",
        "MODEL_NAME = \"finiteautomata/bertweet-base-sentiment-analysis\" # Este modelo ya está afinado específicamente para clasificación de sentimiento en tweets\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Creamos una función para predecir sentimiento\n",
        "def predict_sentiment(tweet: str) -> str:\n",
        "    inputs = tokenizer(tweet, return_tensors=\"pt\", truncation=True)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        label_id = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "    labels = model.config.id2label\n",
        "    return labels[label_id]\n",
        "\n",
        "# Lo aplico a mis tweets en formato original (sin preprocesado) porque el modelo de Hugging Face está entrenado para interpretar todo (emojis, menciones, etc.) como señales de sentimiento\n",
        "tqdm.pandas() # Para mostrar una barra de progreso\n",
        "# Convierto los valores a cadenas y reemplazo los NaN por texto vacío\n",
        "df['Tweet'] = df['Tweet'].fillna('').astype(str)\n",
        "# Creamos una nueva columna llamada bertweet_sentiment con etiquetas como NEG, NEU, POS llamando a la función anterior\n",
        "df['bertweet_sentiment'] = df['Tweet'].progress_apply(predict_sentiment)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "jO9p30Ej_mJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos el resultado\n",
        "df.to_excel(\"analisis_sentimiento_BERTweet.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "NYnLuuvK_7cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Asignamos un valor numérico al sentimiento\n",
        "sentiment_map = {'POS': 1, 'NEU': 0, 'NEG': -1}\n",
        "df['sentiment_score'] = df['bertweet_sentiment'].map(sentiment_map)\n",
        "\n",
        "# Agrupamos por fecha (media diaria del sentimiento)\n",
        "df_sentiment_daily = df.groupby('date')['sentiment_score'].mean().reset_index()\n",
        "\n",
        "df_sentiment_daily.head()"
      ],
      "metadata": {
        "id": "Ze1XO_X6ARnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sentiment_daily.shape"
      ],
      "metadata": {
        "id": "otgy_MyoAVl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.shape"
      ],
      "metadata": {
        "id": "Qm2iEvPdAbxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JOIN: Unimos sentimiento con precios por fecha\n",
        "df_merged = pd.merge(df_precio, df_sentiment_daily, left_on='Date', right_on='date', how='left')\n",
        "\n",
        "# Rellenar días sin tweets con 0 (neutral)\n",
        "df_merged['sentiment_score'] = df_merged['sentiment_score'].fillna(0)\n",
        "\n",
        "# Crear la columna de retorno diario\n",
        "df_merged['daily_return'] = df_merged['Close'].pct_change()\n",
        "df_merged.head()"
      ],
      "metadata": {
        "id": "j99bbe1SAXcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Correlación numérica\n",
        "correlation = df_merged[['sentiment_score', 'daily_return']].corr()\n",
        "print(correlation)"
      ],
      "metadata": {
        "id": "012wm3r3AeOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. La correlación con desfase de 1 día es muy útil porque mide si el sentimiento de hoy tiene algún poder predictivo sobre el retorno de mañana.\n",
        "\n",
        "# ───────────────────────────\n",
        "# Crear columna con el retorno del día siguiente (desfase de 1 día)\n",
        "# ───────────────────────────\n",
        "df_merged['daily_return_lag1'] = df_merged['daily_return'].shift(-1)\n",
        "\n",
        "# ───────────────────────────\n",
        "# Calcular correlación de Pearson entre sentimiento actual y retorno del día siguiente\n",
        "# ───────────────────────────\n",
        "corr_lag1 = df_merged[['sentiment_score', 'daily_return_lag1']].corr().iloc[0,1]\n",
        "\n",
        "print(f\"Correlación (sentimiento_t vs. retorno_t+1): {corr_lag1:.4f}\")"
      ],
      "metadata": {
        "id": "iyWfr8o4AgFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Diagrama de dispersión para ver la forma de la relación\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.regplot(data=df_merged, x='sentiment_score', y='daily_return')\n",
        "plt.title('Sentiment Score de tweets vs Daily Return (Tesla)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bLL5CtGAAiwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Gráfico combinado en el tiempo\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Copia del dataframe para no modificar el original\n",
        "df_plot = df_merged.copy()\n",
        "\n",
        "# Media móvil de 7 días para suavizar el sentimiento\n",
        "df_plot['sentiment_rolling'] = df_plot['sentiment_score'].rolling(7).mean()\n",
        "\n",
        "# Crear figura y ejes\n",
        "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
        "\n",
        "# --- Eje 1: Precio de cierre ---\n",
        "ax1.plot(df_plot['Date'], df_plot['Close'], color='blue', label='Precio de cierre')\n",
        "ax1.set_ylabel('Precio de acción (USD)', color='blue')\n",
        "ax1.tick_params(axis='y', labelcolor='blue')\n",
        "\n",
        "# --- Eje 2: Sentimiento ---\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "# Dibujar el sentimiento diario con colores (verde si >0, rojo si <0)\n",
        "colors = np.where(df_plot['sentiment_score'] >= 0, 'green', 'red')\n",
        "ax2.bar(df_plot['Date'], df_plot['sentiment_score'],\n",
        "        color=colors, alpha=0.3, width=1.0)\n",
        "\n",
        "# Añadir línea suavizada (media móvil 7 días)\n",
        "ax2.plot(df_plot['Date'], df_plot['sentiment_rolling'],\n",
        "         color='orange', linestyle='--', linewidth=2, label='Sentimiento (media móvil 7 días)')\n",
        "\n",
        "ax2.set_ylabel('Sentimiento promedio (sentiment_score)', color='red')\n",
        "ax2.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "# --- Leyenda manual para diferenciar positivo y negativo ---\n",
        "pos_patch = mpatches.Patch(color='green', alpha=0.3, label='Sentimiento positivo')\n",
        "neg_patch = mpatches.Patch(color='red', alpha=0.3, label='Sentimiento negativo')\n",
        "\n",
        "# --- Título y estética ---\n",
        "plt.title('Evolución del precio de TSLA y sentimiento de tweets (2022)', fontsize=14, fontweight='bold')\n",
        "fig.tight_layout()\n",
        "\n",
        "# Leyendas combinadas\n",
        "lines_labels = [ax.get_legend_handles_labels() for ax in [ax1, ax2]]\n",
        "lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
        "# Añadir parches personalizados\n",
        "lines += [pos_patch, neg_patch]\n",
        "labels += [pos_patch.get_label(), neg_patch.get_label()]\n",
        "\n",
        "ax1.legend(lines, labels, loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wXkrtpdPAnPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Agrupación semanal para mostrar cómo evoluciona el sentimiento semanal frente al precio semanal\n",
        "\n",
        "# Agrupar semanalmente promediando valores numéricos\n",
        "df_weekly = df_merged.set_index('Date').resample('W').mean(numeric_only=True).reset_index()\n",
        "\n",
        "# Calcular correlación semanal\n",
        "correlacion_semanal = df_weekly[['sentiment_score', 'daily_return']].corr().iloc[0, 1]\n",
        "print(f\"📅 Correlación semanal entre sentimiento y retorno: {correlacion_semanal:.4f}\")"
      ],
      "metadata": {
        "id": "fIAyWcLTAxNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Vamos a comprobar si el sentimiento de la semana anterior predice el retorno de la semana actual.\n",
        "# Esto es muy útil para evaluar si el sentimiento anticipa movimientos de precio.\n",
        "# Para ello aplicamos un desfase (lag).\n",
        "\n",
        "# Crear una nueva columna con el sentimiento de la semana anterior\n",
        "df_weekly['sentiment_lag1'] = df_weekly['sentiment_score'].shift(1)\n",
        "\n",
        "# Calcular correlación entre sentimiento lag y retorno actual\n",
        "correlacion_lag1 = df_weekly[['sentiment_lag1', 'daily_return']].corr().iloc[0, 1]\n",
        "print(f\"📊 Correlación con desfase 1 semana: {correlacion_lag1:.4f}\")"
      ],
      "metadata": {
        "id": "2LhCS_7eA04c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Análisis comparativo MISMO DÍA y DÍA SIGUIENTE\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Asegurarse de que el DataFrame esté ordenado por fecha\n",
        "df_merged = df_merged.sort_values(by='Date')\n",
        "\n",
        "# ────────────────\n",
        "# Desfase de 1 día\n",
        "# ────────────────\n",
        "df_merged['daily_return_next'] = df_merged['daily_return'].shift(-1)\n",
        "\n",
        "# ────────────────\n",
        "# Función para calcular porcentaje de coincidencia\n",
        "# ────────────────\n",
        "def porcentaje_coincidencia(df, sentiment_col, return_col):\n",
        "    positivos = df[df[sentiment_col] > 0]\n",
        "    negativos = df[df[sentiment_col] < 0]\n",
        "\n",
        "    subidas_positivas = positivos[positivos[return_col] > 0]\n",
        "    bajadas_negativas = negativos[negativos[return_col] < 0]\n",
        "\n",
        "    porcentaje_pos = len(subidas_positivas) / len(positivos) * 100 if len(positivos) > 0 else 0\n",
        "    porcentaje_neg = len(bajadas_negativas) / len(negativos) * 100 if len(negativos) > 0 else 0\n",
        "\n",
        "    tabla = pd.DataFrame({\n",
        "        'Sentimiento': ['Positivo', 'Negativo'],\n",
        "        'Días totales': [len(positivos), len(negativos)],\n",
        "        'Mov. esperado': [len(subidas_positivas), len(bajadas_negativas)],\n",
        "        '% coincidencia': [porcentaje_pos, porcentaje_neg]\n",
        "    })\n",
        "    return tabla\n",
        "\n",
        "# ────────────────\n",
        "# 1. Coincidencia mismo día\n",
        "# ────────────────\n",
        "tabla_mismo_dia = porcentaje_coincidencia(df_merged, 'sentiment_score', 'daily_return')\n",
        "tabla_mismo_dia['Escenario'] = 'Mismo día'\n",
        "\n",
        "# ────────────────\n",
        "# 2. Coincidencia día siguiente\n",
        "# ────────────────\n",
        "tabla_dia_siguiente = porcentaje_coincidencia(df_merged, 'sentiment_score', 'daily_return_next')\n",
        "tabla_dia_siguiente['Escenario'] = 'Día siguiente'\n",
        "\n",
        "# ────────────────\n",
        "# Combinar tablas\n",
        "# ────────────────\n",
        "tabla_completa = pd.concat([tabla_mismo_dia, tabla_dia_siguiente], ignore_index=True)\n",
        "print(tabla_completa)\n",
        "\n",
        "# ────────────────\n",
        "# Gráfico comparativo\n",
        "# ────────────────\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "anchura = 0.35\n",
        "x = range(len(tabla_mismo_dia))\n",
        "\n",
        "# Barras mismo día\n",
        "plt.bar([i - anchura/2 for i in x],\n",
        "        tabla_mismo_dia['% coincidencia'],\n",
        "        width=anchura,\n",
        "        color=['green','red'],\n",
        "        alpha=0.7,\n",
        "        label='Mismo día')\n",
        "\n",
        "# Barras día siguiente\n",
        "plt.bar([i + anchura/2 for i in x],\n",
        "        tabla_dia_siguiente['% coincidencia'],\n",
        "        width=anchura,\n",
        "        color=['darkgreen','darkred'],\n",
        "        alpha=0.7,\n",
        "        label='Día siguiente')\n",
        "\n",
        "plt.xticks(ticks=x, labels=tabla_mismo_dia['Sentimiento'])\n",
        "plt.ylabel('% días en que el precio se movió según el sentimiento')\n",
        "plt.title('Comparación de coincidencia entre sentimiento y movimiento del precio')\n",
        "plt.ylim(0, 100)\n",
        "plt.legend()\n",
        "\n",
        "# Añadir etiquetas de porcentaje encima de las barras\n",
        "for i, val in enumerate(tabla_mismo_dia['% coincidencia']):\n",
        "    plt.text(i - anchura/2, val + 2, f\"{val:.1f}%\", ha='center', fontsize=10)\n",
        "for i, val in enumerate(tabla_dia_siguiente['% coincidencia']):\n",
        "    plt.text(i + anchura/2, val + 2, f\"{val:.1f}%\", ha='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7y9A3RuTBHe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Análisis comparativo SENTIMIENTO EXTREMO\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Asegurarse de que el DataFrame esté ordenado por fecha\n",
        "df_merged = df_merged.sort_values(by='Date')\n",
        "\n",
        "# ────────────────\n",
        "# Desfase de 1 día\n",
        "# ────────────────\n",
        "df_merged['daily_return_next'] = df_merged['daily_return'].shift(-1)\n",
        "\n",
        "# ────────────────\n",
        "# Función para calcular coincidencia según umbrales de sentimiento extremo\n",
        "# ────────────────\n",
        "def porcentaje_coincidencia_extremo(df, sentiment_col, return_col, umbral_pos=0.2, umbral_neg=-0.2):\n",
        "    positivos = df[df[sentiment_col] > umbral_pos]\n",
        "    negativos = df[df[sentiment_col] < umbral_neg]\n",
        "\n",
        "    subidas_positivas = positivos[positivos[return_col] > 0]\n",
        "    bajadas_negativas = negativos[negativos[return_col] < 0]\n",
        "\n",
        "    porcentaje_pos = len(subidas_positivas) / len(positivos) * 100 if len(positivos) > 0 else 0\n",
        "    porcentaje_neg = len(bajadas_negativas) / len(negativos) * 100 if len(negativos) > 0 else 0\n",
        "\n",
        "    tabla = pd.DataFrame({\n",
        "        'Sentimiento': ['Muy positivo', 'Muy negativo'],\n",
        "        'Días totales': [len(positivos), len(negativos)],\n",
        "        'Mov. esperado': [len(subidas_positivas), len(bajadas_negativas)],\n",
        "        '% coincidencia': [porcentaje_pos, porcentaje_neg]\n",
        "    })\n",
        "    return tabla\n",
        "\n",
        "# ────────────────\n",
        "# 1. Coincidencia mismo día\n",
        "# ────────────────\n",
        "tabla_mismo_dia_extremo = porcentaje_coincidencia_extremo(df_merged, 'sentiment_score', 'daily_return')\n",
        "tabla_mismo_dia_extremo['Escenario'] = 'Mismo día'\n",
        "\n",
        "# ────────────────\n",
        "# 2. Coincidencia día siguiente\n",
        "# ────────────────\n",
        "tabla_dia_siguiente_extremo = porcentaje_coincidencia_extremo(df_merged, 'sentiment_score', 'daily_return_next')\n",
        "tabla_dia_siguiente_extremo['Escenario'] = 'Día siguiente'\n",
        "\n",
        "# ────────────────\n",
        "# Combinar tablas\n",
        "# ────────────────\n",
        "tabla_completa_extremo = pd.concat([tabla_mismo_dia_extremo, tabla_dia_siguiente_extremo], ignore_index=True)\n",
        "print(tabla_completa_extremo)\n",
        "\n",
        "# ────────────────\n",
        "# Gráfico comparativo\n",
        "# ────────────────\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "anchura = 0.35\n",
        "x = range(len(tabla_mismo_dia_extremo))\n",
        "\n",
        "# Barras mismo día\n",
        "plt.bar([i - anchura/2 for i in x],\n",
        "        tabla_mismo_dia_extremo['% coincidencia'],\n",
        "        width=anchura,\n",
        "        color=['green','red'],\n",
        "        alpha=0.7,\n",
        "        label='Mismo día')\n",
        "\n",
        "# Barras día siguiente\n",
        "plt.bar([i + anchura/2 for i in x],\n",
        "        tabla_dia_siguiente_extremo['% coincidencia'],\n",
        "        width=anchura,\n",
        "        color=['darkgreen','darkred'],\n",
        "        alpha=0.7,\n",
        "        label='Día siguiente')\n",
        "\n",
        "plt.xticks(ticks=x, labels=tabla_mismo_dia_extremo['Sentimiento'])\n",
        "plt.ylabel('% días en que el precio se movió según el sentimiento extremo')\n",
        "plt.title('Relación entre sentimientos extremos y movimiento del precio de las acciones')\n",
        "plt.ylim(0, 100)\n",
        "plt.legend()\n",
        "\n",
        "# Añadir etiquetas de porcentaje encima de las barras\n",
        "for i, val in enumerate(tabla_mismo_dia_extremo['% coincidencia']):\n",
        "    plt.text(i - anchura/2, val + 2, f\"{val:.1f}%\", ha='center', fontsize=10)\n",
        "for i, val in enumerate(tabla_dia_siguiente_extremo['% coincidencia']):\n",
        "    plt.text(i + anchura/2, val + 2, f\"{val:.1f}%\", ha='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tAP8rju8BYU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.\tTweets por usuario específico vs Precio de acciones"
      ],
      "metadata": {
        "id": "hGjnHqsEBq-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Elon Musk Tweets 2010 to 2025**\n",
        "\n",
        "URL: https://www.kaggle.com/datasets/dadalyndell/elon-musk-tweets-2010-to-2025-march?select=all_musk_posts.csv"
      ],
      "metadata": {
        "id": "4nPliVoaCUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Cargo el dataset con los tweets de Elon Musk entre el periodo de 2010 a 04/2025\n",
        "df_tweets_ElonMusk = pd.read_csv('all_musk_posts.csv')"
      ],
      "metadata": {
        "id": "JvKWEukoCmw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tweets_ElonMusk.head()"
      ],
      "metadata": {
        "id": "CIYnuxTVCpwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tweets_ElonMusk.shape"
      ],
      "metadata": {
        "id": "_8-GN931CriL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tweets_ElonMusk.info()"
      ],
      "metadata": {
        "id": "WhNN0fPpCs6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#                Búsqueda de nulos por columnas del Dataframe                  #\n",
        "################################################################################\n",
        "\n",
        "# Para poder encontrar el total de valores nulos haremos un tratamiento por columnas y, sumando dicho valor, obtendremos el total de nulos en cada columna\n",
        "nulosPorColumna = df_tweets_ElonMusk.isnull().sum()\n",
        "\n",
        "#Mostramos el resultado en una tabla\n",
        "print(\"Número de valores nulos por columna:\\n\" + str(nulosPorColumna))"
      ],
      "metadata": {
        "id": "s8cZBvCeCwDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualizamos los valores nulos con Heatmap para una representación gráfica de los valores nulos\n",
        "# Elegimos el color verde para representar que las columnas contienen un valor y en color rojo la representación de los valores nulos\n",
        "sns.heatmap(df_tweets_ElonMusk.isnull(), cbar=False, cmap='RdYlGn_r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "twy9rNYuCy3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Para calcular el engagement_score, la mejor práctica es asumir que los valores nulos son ceros,\n",
        "porque si no hay datos de interacción es probable que no haya habido interacciones o no se registraron. \"\"\"\n",
        "# Relleno con ceros\n",
        "cols_to_fill = ['retweetCount', 'replyCount', 'likeCount', 'quoteCount', 'viewCount', 'bookmarkCount']\n",
        "df_tweets_ElonMusk[cols_to_fill] = df_tweets_ElonMusk[cols_to_fill].fillna(0)\n",
        "\n",
        "nulosPorColumna = df_tweets_ElonMusk.isnull().sum()\n",
        "print(\"Número de valores nulos por columna:\\n\" + str(nulosPorColumna))"
      ],
      "metadata": {
        "id": "jGhMLtNvC3Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculamos el engagement de cada tweet\n",
        "\n",
        "def calcula_engagement_score(row):\n",
        "    # Filtro los tweets originales creados por Elon Musk (no me interesan los que él retuitea)\n",
        "    if row['isRetweet']:\n",
        "        return 0\n",
        "\n",
        "    return round(\n",
        "        2 * row['retweetCount'] +\n",
        "        1.5 * row['replyCount'] +\n",
        "        1.2 * row['likeCount'] +\n",
        "        0.001 * row['viewCount'],\n",
        "        2\n",
        "    )\n",
        "\n",
        "# Aplica la función a cada fila del DataFrame\n",
        "df_tweets_ElonMusk['engagement_score'] = df_tweets_ElonMusk.apply(calcula_engagement_score, axis=1)\n",
        "\n",
        "df_tweets_ElonMusk.head()"
      ],
      "metadata": {
        "id": "c7HqkVOcC9nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtro los tweets más influyentes\n",
        "# Por ejemplo, ordeno por engagement_score de mayor a menor\n",
        "df_tweets_ElonMusk_ordenado = df_tweets_ElonMusk.sort_values(by='engagement_score', ascending=False)\n",
        "\n",
        "# Muestro los 10 tweets con más engagement\n",
        "df_tweets_ElonMusk_ordenado[['createdAt', 'fullText', 'engagement_score']].head(10)"
      ],
      "metadata": {
        "id": "7lOUyA0FDPOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtramos por palabras clave por empresa/activo para buscar patrones con el movimiento del precio de acciones\n",
        "keywords_tsla = ['tesla', 'tsla']\n",
        "keywords_btc = ['bitcoin', 'btc']\n",
        "keywords_doge = ['doge', 'dogecoin']\n",
        "\n",
        "# Busca menciones relacionadas\n",
        "tsla_tweets = df_tweets_ElonMusk_ordenado[df_tweets_ElonMusk_ordenado['fullText'].str.lower().str.contains('|'.join(keywords_tsla))]\n",
        "btc_tweets = df_tweets_ElonMusk_ordenado[df_tweets_ElonMusk_ordenado['fullText'].str.lower().str.contains('|'.join(keywords_btc))]\n",
        "doge_tweets = df_tweets_ElonMusk_ordenado[df_tweets_ElonMusk_ordenado['fullText'].str.lower().str.contains('|'.join(keywords_doge))]"
      ],
      "metadata": {
        "id": "xUMLdh1sDZ5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doge_tweets[['engagement_score', 'fullText', 'createdAt']].head()"
      ],
      "metadata": {
        "id": "8Ul1kKZ9Dbso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doge_tweets.shape"
      ],
      "metadata": {
        "id": "Je0-aZQdDfod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertimos la columna \"createdAt\", que es de tipo objeto a tipo datetime\n",
        "doge_tweets['createdAt'] = pd.to_datetime(doge_tweets['createdAt'])\n",
        "\n",
        "# Crear nueva columna 'date' con solo la fecha (sin hora)\n",
        "doge_tweets['date'] = doge_tweets['createdAt'].dt.date\n",
        "\n",
        "# Convertimos la columna \"date\", que es de tipo objeto a tipo datetime\n",
        "doge_tweets['date'] = pd.to_datetime(doge_tweets['date'])\n",
        "\n",
        "doge_tweets.info()"
      ],
      "metadata": {
        "id": "ybzGvSdqDliR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupamos los engagement_score por fecha (por si hay varios tweets el mismo día) y se acumula el engagement_score diario\n",
        "df_engagement_daily = doge_tweets.groupby('date')['engagement_score'].sum().reset_index()\n",
        "\n",
        "df_engagement_daily.head(10)"
      ],
      "metadata": {
        "id": "lOUFy_MyDo7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_engagement_daily.shape"
      ],
      "metadata": {
        "id": "enMLNsqlDsjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.shape"
      ],
      "metadata": {
        "id": "699KXeGHDx9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JOIN: Unimos engagement con precios por fecha\n",
        "df_merged = pd.merge(df_precio, df_engagement_daily, left_on='Date', right_on='date', how='left')\n",
        "\n",
        "# Rellenar días sin tweets publicados por ElonMusk relacionados con DOGE con 0 (engagement nulo)\n",
        "df_merged['engagement_score'] = df_merged['engagement_score'].fillna(0)\n",
        "\n",
        "# Crear la columna de retorno diario\n",
        "df_merged['daily_return'] = df_merged['Close'].pct_change()\n",
        "df_merged.head()"
      ],
      "metadata": {
        "id": "kgTl9kbRDuUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Correlación numérica\n",
        "correlation = df_merged[['engagement_score', 'daily_return']].corr()\n",
        "print(correlation)"
      ],
      "metadata": {
        "id": "kbgU4-MGDvuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Gráfico de dispersión (scatter plot)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df_merged, x='engagement_score', y='daily_return')\n",
        "plt.title('Correlación entre engagement_score de tweets de Elon Musk y retorno diario de DOGE')\n",
        "plt.xlabel('engagement_score tweets')\n",
        "plt.ylabel('Retorno diario (%)')\n",
        "plt.axhline(0, color='gray', linestyle='--')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LAsFiHcID08s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Gráfico combinado en el tiempo\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Copia del dataframe para no modificar el original\n",
        "df_plot = df_merged.copy()\n",
        "\n",
        "# Media móvil de 7 días para suavizar el engagement\n",
        "df_plot['engagement_rolling'] = df_plot['engagement_score'].rolling(7).mean()\n",
        "\n",
        "# Crear figura y ejes\n",
        "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
        "\n",
        "# --- Eje 1: Precio de cierre ---\n",
        "ax1.plot(df_plot['Date'], df_plot['Close'], color='#1f77b4', linewidth=2, label='Precio de cierre')\n",
        "ax1.set_ylabel('Precio de acción (USD)', color='#1f77b4', fontsize=12)\n",
        "ax1.tick_params(axis='y', labelcolor='#1f77b4')\n",
        "ax1.grid(axis='x', linestyle='--', alpha=0.3)\n",
        "\n",
        "# --- Eje 2: Engagement ---\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "# Línea de engagement diario más visible\n",
        "ax2.plot(df_plot['Date'], df_plot['engagement_score'],\n",
        "         color='red', linewidth=1.5, alpha=0.3, label='Engagement')\n",
        "\n",
        "# Línea de media móvil discontinua\n",
        "ax2.plot(df_plot['Date'], df_plot['engagement_rolling'],\n",
        "         color='#ff7f0e', linestyle='--', linewidth=2.5, label='Engagement (media móvil 7 días)')\n",
        "\n",
        "ax2.set_ylabel('Engagement score', color='red', fontsize=12)\n",
        "ax2.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "# --- Título y estética ---\n",
        "plt.title('Evolución del precio de Dogecoin vs engagement de tweets de Elon Musk relacionados con la criptomoneda',\n",
        "          fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "# Leyendas combinadas\n",
        "lines_labels = [ax.get_legend_handles_labels() for ax in [ax1, ax2]]\n",
        "lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
        "ax1.legend(lines, labels, loc='upper center', frameon=True, fontsize=11, facecolor='white', edgecolor='gray')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EreX8o7rD84c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Agrupación semanal para mostrar cómo evoluciona el engagement semanal frente al precio semanal\n",
        "\n",
        "# Agrupar semanalmente promediando valores numéricos\n",
        "df_weekly = df_merged.set_index('Date').resample('W').mean(numeric_only=True).reset_index()\n",
        "\n",
        "# Calcular correlación semanal\n",
        "correlacion_semanal = df_weekly[['engagement_score', 'daily_return']].corr().iloc[0, 1]\n",
        "print(f\"📅 Correlación semanal entre engagement y retorno: {correlacion_semanal:.4f}\")"
      ],
      "metadata": {
        "id": "fY_yVgQAEGjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Vamos a comprobar si el engagement de la semana anterior predice el retorno de la semana actual.\n",
        "# Esto es muy útil para evaluar si el engagement anticipa movimientos de precio.\n",
        "# Para ello aplicamos un desfase (lag).\n",
        "\n",
        "# Crear una nueva columna con el engagement de la semana anterior\n",
        "df_weekly['engagement_lag1'] = df_weekly['engagement_score'].shift(1)\n",
        "\n",
        "# Calcular correlación entre engagement lag y retorno actual\n",
        "correlacion_lag1 = df_weekly[['engagement_lag1', 'daily_return']].corr().iloc[0, 1]\n",
        "print(f\"📊 Correlación con desfase 1 semana: {correlacion_lag1:.4f}\")"
      ],
      "metadata": {
        "id": "awh4VrkwELFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Calcular variación de precio (día siguiente)\n",
        "df_merged['next_day_price'] = df_merged['Close'].shift(-1)\n",
        "df_merged['price_change'] = df_merged['next_day_price'] - df_merged['Close']\n",
        "\n",
        "# Correlación\n",
        "corr = df_merged[['engagement_score', 'price_change']].corr()\n",
        "print(corr)"
      ],
      "metadata": {
        "id": "u-VwbT7JEOQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Calcular variación de precio a 2 días después\n",
        "df_merged['price_change_2d'] = df_merged['Close'].shift(-2) - df_merged['Close']\n",
        "print(df_merged[['engagement_score', 'price_change_2d']].corr())"
      ],
      "metadata": {
        "id": "jjhw5ZfSEQG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. A veces, tweets con mucho engagement aumentan la volatilidad, no necesariamente la dirección:\n",
        "df = df_merged.copy()\n",
        "\n",
        "# Asegurar orden temporal\n",
        "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
        "\n",
        "# Volatilidad: rango intradía (High-Low) / Close\n",
        "df[\"volatility\"] = (df[\"High\"] - df[\"Low\"]) / df[\"Close\"]\n",
        "\n",
        "# Seleccionar columnas de interés para el cálculo de la correlación\n",
        "corr_df = df[['engagement_score', 'volatility', 'Volume']]\n",
        "\n",
        "# Correlación de Pearson\n",
        "corr_matrix = corr_df.corr()\n",
        "print(corr_matrix)"
      ],
      "metadata": {
        "id": "JWBN0vPtESoz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}